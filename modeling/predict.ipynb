{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9da9aea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "IE_project 데이터베이스 설정\n",
      "============================================================\n",
      "✓ IE_project 데이터베이스 생성/선택 완료\n",
      "✓ model_output 테이블 생성/확인 완료\n",
      "✓ 총 테이블 수: 4\n",
      "테이블 목록: ['dart_data', 'ecos_data', 'final_features', 'model_output']\n",
      "✓ IE_project 데이터베이스 연결 테스트 성공!\n",
      "\n",
      "🎉 IE_project 데이터베이스 설정 완료!\n",
      "이제 predict.ipynb에서 IE_project DB를 사용할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# IE_project 데이터베이스 사용 및 model_output 테이블 생성\n",
    "import os\n",
    "import sys\n",
    "import mysql.connector\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# DB 폴더의 .env 파일 로드\n",
    "db_folder = os.path.join(os.path.dirname(os.getcwd()), 'DB')\n",
    "env_path = os.path.join(db_folder, '.env')\n",
    "load_dotenv(env_path)\n",
    "\n",
    "def setup_ie_project_db():\n",
    "    \"\"\"IE_project 데이터베이스 설정 및 테이블 생성\"\"\"\n",
    "    try:\n",
    "        # 먼저 데이터베이스 없이 연결\n",
    "        config_no_db = {\n",
    "            'host': 'localhost',\n",
    "            'user': 'root',\n",
    "            'password': 'Woorifisa5!',\n",
    "            'charset': 'utf8mb4'\n",
    "        }\n",
    "        \n",
    "        connection = mysql.connector.connect(**config_no_db)\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # IE_project 데이터베이스 생성 및 사용\n",
    "        cursor.execute(\"CREATE DATABASE IF NOT EXISTS IE_project\")\n",
    "        cursor.execute(\"USE IE_project\")\n",
    "        print(\"✓ IE_project 데이터베이스 생성/선택 완료\")\n",
    "        \n",
    "        # model_output 테이블 생성 (누락된 테이블)\n",
    "        model_output_ddl = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS model_output (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            date VARCHAR(6) NOT NULL,\n",
    "            construction_bsi_actual DECIMAL(5, 1),\n",
    "            base_rate DECIMAL(5, 3),\n",
    "            housing_sale_price DECIMAL(8, 3),\n",
    "            m2_growth DECIMAL(8, 2),\n",
    "            credit_spread DECIMAL(8, 3),\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n",
    "            UNIQUE KEY unique_date (date)\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(model_output_ddl)\n",
    "        print(\"✓ model_output 테이블 생성/확인 완료\")\n",
    "        \n",
    "        # 테이블 목록 확인\n",
    "        cursor.execute(\"SHOW TABLES\")\n",
    "        tables = cursor.fetchall()\n",
    "        table_names = [table[0] for table in tables]\n",
    "        print(f\"✓ 총 테이블 수: {len(tables)}\")\n",
    "        print(f\"테이블 목록: {table_names}\")\n",
    "        \n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        \n",
    "        # IE_project 데이터베이스로 연결 테스트\n",
    "        config_with_db = {\n",
    "            'host': 'localhost',\n",
    "            'user': 'root',\n",
    "            'password': 'Woorifisa5!',\n",
    "            'database': 'IE_project',\n",
    "            'charset': 'utf8mb4'\n",
    "        }\n",
    "        \n",
    "        connection = mysql.connector.connect(**config_with_db)\n",
    "        print(\"✓ IE_project 데이터베이스 연결 테스트 성공!\")\n",
    "        connection.close()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"데이터베이스 설정 오류: {e}\")\n",
    "        return False\n",
    "\n",
    "# IE_project 데이터베이스 설정\n",
    "print(\"=\"*60)\n",
    "print(\"IE_project 데이터베이스 설정\")\n",
    "print(\"=\"*60)\n",
    "success = setup_ie_project_db()\n",
    "if success:\n",
    "    print(\"\\n🎉 IE_project 데이터베이스 설정 완료!\")\n",
    "    print(\"이제 predict.ipynb에서 IE_project DB를 사용할 수 있습니다.\")\n",
    "else:\n",
    "    print(\"\\n❌ 데이터베이스 설정 실패!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3ecf7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TASK 1: 라이브러리 import 및 환경 설정\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac31ac8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (187, 28)\n",
      "Available target columns: ['construction_bsi_actual', 'base_rate', 'housing_sale_price', 'm2_growth', 'credit_spread']\n",
      "Original features: 67\n",
      "Final selected features: 26\n",
      "Features shape: (180, 26)\n",
      "Targets shape: (180, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# TASK 2: 데이터 로드 및 피쳐 엔지니어링\n",
    "# ============================================================\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv('ecos_monthly_data.csv')\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "\n",
    "# 날짜 컬럼 처리\n",
    "if 'date' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').set_index('date')\n",
    "elif 'Date' in df.columns:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values('Date').set_index('Date')\n",
    "\n",
    "# 타겟 변수 정의\n",
    "target_columns = [\n",
    "    'construction_bsi_actual',\n",
    "    'base_rate', \n",
    "    'housing_sale_price',\n",
    "    'm2_growth',\n",
    "    'credit_spread'\n",
    "]\n",
    "\n",
    "available_targets = [col for col in target_columns if col in df.columns]\n",
    "print(f\"Available target columns: {available_targets}\")\n",
    "\n",
    "# 결측치 처리\n",
    "df = df.interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "# 타겟 변수 차분 적용 (비정상성 제거)\n",
    "for col in available_targets:\n",
    "    df[f'{col}_diff'] = df[col].diff()\n",
    "\n",
    "diff_targets = [f'{col}_diff' for col in available_targets]\n",
    "\n",
    "# 피쳐 엔지니어링: 이동평균, 지연, 변화율\n",
    "for col in available_targets:\n",
    "    diff_col = f'{col}_diff'\n",
    "    \n",
    "    # 이동평균\n",
    "    df[f'{diff_col}_ma3'] = df[diff_col].rolling(window=3, min_periods=1).mean()\n",
    "    df[f'{diff_col}_ma6'] = df[diff_col].rolling(window=6, min_periods=1).mean()\n",
    "    \n",
    "    # 변화율\n",
    "    df[f'{diff_col}_pct_change'] = df[diff_col].pct_change().fillna(0)\n",
    "    \n",
    "    # 지연 특징\n",
    "    for lag in [1, 3, 6]:\n",
    "        df[f'{diff_col}_lag{lag}'] = df[diff_col].shift(lag)\n",
    "        df[f'{col}_lag{lag}'] = df[col].shift(lag)\n",
    "\n",
    "# 결측치 제거\n",
    "df = df.dropna()\n",
    "\n",
    "# 피쳐 선택: 상관관계 기반\n",
    "all_features = [col for col in df.columns if col not in available_targets and col not in diff_targets]\n",
    "\n",
    "# 높은 상관관계 피쳐 제거\n",
    "def remove_highly_correlated_features(corr_matrix, threshold=0.95):\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper_tri.columns if any(abs(upper_tri[column]) > threshold)]\n",
    "    return to_drop\n",
    "\n",
    "correlation_matrix = df[all_features].corr()\n",
    "highly_corr_features = remove_highly_correlated_features(correlation_matrix)\n",
    "selected_features = [col for col in all_features if col not in highly_corr_features]\n",
    "\n",
    "# 타겟과의 상관관계 기반 최종 피쳐 선택\n",
    "target_correlations = []\n",
    "for target in diff_targets:\n",
    "    if target in df.columns:\n",
    "        corr_with_target = df[selected_features + [target]].corr()[target].abs().sort_values(ascending=False)\n",
    "        target_correlations.append(corr_with_target[:-1])\n",
    "\n",
    "avg_correlation = pd.concat(target_correlations, axis=1).mean(axis=1).sort_values(ascending=False)\n",
    "n_features = max(20, int(len(selected_features) * 0.5))\n",
    "final_features = avg_correlation.head(n_features).index.tolist()\n",
    "\n",
    "print(f\"Original features: {len(all_features)}\")\n",
    "print(f\"Final selected features: {len(final_features)}\")\n",
    "\n",
    "# 최종 데이터 준비\n",
    "X = df[final_features].values\n",
    "y = df[diff_targets].values\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Targets shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea0e3c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB 모듈 로드 성공: c:\\Users\\baesh\\Desktop\\kor-ie-proj\\DB\n",
      "DB 설정 로드: c:\\Users\\baesh\\Desktop\\kor-ie-proj\\DB\\.env\n",
      "MySQL 데이터베이스 연결 성공\n",
      "✓ 데이터베이스 연결 성공!\n",
      "\n",
      "============================================================\n",
      "최종 피쳐 DB 저장\n",
      "============================================================\n",
      "피쳐 데이터 180건 저장 완료\n",
      "DB 저장 완료\n",
      "피쳐 데이터 180건 저장 완료\n",
      "DB 저장 완료\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TASK 3: 데이터베이스 연결 및 피쳐 저장\n",
    "# ============================================================\n",
    "\n",
    "# DB 모듈 import\n",
    "try:\n",
    "    from db_query import DatabaseConnection\n",
    "except ImportError:\n",
    "    # 노트북에서는 상대 경로를 직접 지정\n",
    "    import os\n",
    "    import sys\n",
    "    \n",
    "    # modeling 폴더에서 상위 폴더의 DB 폴더로 이동\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "    db_path = os.path.join(parent_dir, 'DB')\n",
    "    \n",
    "    if db_path not in sys.path:\n",
    "        sys.path.insert(0, db_path)\n",
    "    \n",
    "    try:\n",
    "        from db_query import DatabaseConnection\n",
    "        print(f\"DB 모듈 로드 성공: {db_path}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"DB 모듈 로드 실패: {e}\")\n",
    "        DatabaseConnection = None\n",
    "\n",
    "# DB 폴더의 .env 파일 로드\n",
    "from dotenv import load_dotenv\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "db_folder = os.path.join(parent_dir, 'DB')\n",
    "env_path = os.path.join(db_folder, '.env')\n",
    "load_dotenv(env_path)\n",
    "print(f\"DB 설정 로드: {env_path}\")\n",
    "\n",
    "# DB 연결 생성\n",
    "db_connection = None\n",
    "if DatabaseConnection:\n",
    "    try:\n",
    "        # DatabaseConnection 클래스 인스턴스 생성 (DB 폴더의 .env 사용)\n",
    "        db_connection = DatabaseConnection()\n",
    "        if db_connection.connect():\n",
    "            print(\"✓ 데이터베이스 연결 성공!\")\n",
    "        else:\n",
    "            print(\"✗ 데이터베이스 연결 실패\")\n",
    "            db_connection = None\n",
    "    except Exception as e:\n",
    "        print(f\"DB 연결 오류: {e}\")\n",
    "        db_connection = None\n",
    "\n",
    "# 최종 피쳐를 DB에 저장\n",
    "def save_features_to_db(df_data, db_conn):\n",
    "    \"\"\"최종 피쳐를 final_features 테이블에 저장\"\"\"\n",
    "    if not db_conn:\n",
    "        print(\"DB 연결 없음 - 파일로만 저장\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # 데이터 준비\n",
    "        df_save = df_data.copy()\n",
    "        df_save.reset_index(inplace=True)\n",
    "        df_save['date'] = pd.to_datetime(df_save['date']).dt.strftime('%Y%m')\n",
    "        \n",
    "        # DDL 스키마에 맞는 컬럼만 선택\n",
    "        schema_columns = [\n",
    "            'date', 'construction_bsi_actual_diff', 'housing_sale_price_diff', \n",
    "            'm2_growth_diff', 'credit_spread_diff', 'base_rate_diff',\n",
    "            'construction_bsi_mom', 'housing_sale_price_diff_ma3', 'm2_growth_lag1',\n",
    "            'base_rate_mdiff_bp', 'credit_spread_diff_ma3', 'construction_bsi_ma3',\n",
    "            'leading_index', 'housing_sale_price_diff_lag6', 'construction_bsi_actual_lag3',\n",
    "            'construction_bsi_actual_diff_ma3', 'base_rate_diff_ma6', 'term_spread',\n",
    "            'construction_bsi_actual_diff_ma6', 'credit_spread_diff_lag1',\n",
    "            'market_rate_treasury_bond_3yr', 'credit_spread_diff_ma6', 'base_rate_diff_ma3',\n",
    "            'base_rate_lag1', 'esi', 'base_rate_diff_lag3', 'm2_growth_diff_ma6'\n",
    "        ]\n",
    "        \n",
    "        available_cols = [col for col in schema_columns if col in df_save.columns]\n",
    "        df_final = df_save[available_cols].copy()\n",
    "        \n",
    "        success = db_conn.save_final_features(df_final)\n",
    "        return success\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"DB 저장 실패: {e}\")\n",
    "        return False\n",
    "\n",
    "# 피쳐 저장 실행\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"최종 피쳐 DB 저장\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "save_success = save_features_to_db(df, db_connection)\n",
    "if save_success:\n",
    "    print(\"DB 저장 완료\")\n",
    "else:\n",
    "    print(\"DB 저장 실패 - in-memory 데이터로 계속 진행\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9110b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TASK 4: 데이터 준비 함수 및 LSTM 모델 정의\n",
    "# ============================================================\n",
    "\n",
    "def create_sequences(X, y, seq_length):\n",
    "    \"\"\"시계열 데이터를 LSTM 입력용 시퀀스로 변환\"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    for i in range(len(X) - seq_length):\n",
    "        X_seq.append(X[i:i + seq_length])\n",
    "        y_seq.append(y[i + seq_length])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "def prepare_data_no_leakage(X, y, seq_length, batch_size, test_size=0.2, val_size=0.1):\n",
    "    \"\"\"스케일링 누출을 방지한 데이터 준비\"\"\"\n",
    "    # 시퀀스 생성\n",
    "    X_seq, y_seq = create_sequences(X, y, seq_length)\n",
    "    \n",
    "    # 시간 순서 유지하여 분할\n",
    "    total_samples = len(X_seq)\n",
    "    test_start = int(total_samples * (1 - test_size))\n",
    "    val_start = int(test_start * (1 - val_size))\n",
    "    \n",
    "    X_train = X_seq[:val_start]\n",
    "    y_train = y_seq[:val_start]\n",
    "    X_val = X_seq[val_start:test_start]\n",
    "    y_val = y_seq[val_start:test_start]\n",
    "    X_test = X_seq[test_start:]\n",
    "    y_test = y_seq[test_start:]\n",
    "    \n",
    "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "    \n",
    "    # 스케일링 (train에서만 fit)\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
    "    y_train_flat = y_train.reshape(-1, y_train.shape[-1])\n",
    "    \n",
    "    scaler_X.fit(X_train_flat)\n",
    "    scaler_y.fit(y_train_flat)\n",
    "    \n",
    "    # 스케일링 적용\n",
    "    X_train_scaled = np.array([scaler_X.transform(seq) for seq in X_train])\n",
    "    X_val_scaled = np.array([scaler_X.transform(seq) for seq in X_val])\n",
    "    X_test_scaled = np.array([scaler_X.transform(seq) for seq in X_test])\n",
    "    \n",
    "    y_train_scaled = scaler_y.transform(y_train)\n",
    "    y_val_scaled = scaler_y.transform(y_val)\n",
    "    y_test_scaled = scaler_y.transform(y_test)\n",
    "    \n",
    "    # Tensor 변환\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train_scaled).to(device)\n",
    "    X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "    y_val_tensor = torch.FloatTensor(y_val_scaled).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    y_test_tensor = torch.FloatTensor(y_test_scaled).to(device)\n",
    "    \n",
    "    # DataLoader 생성\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, scaler_X, scaler_y\n",
    "\n",
    "class MultivariateLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_rate=0.2):\n",
    "        super(MultivariateLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        output = self.dropout(last_output)\n",
    "        output = self.fc(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11761c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:13:50,273] A new study created in memory with name: no-name-f1e51dbf-6f7c-4214-a08b-601b0f67c913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "하이퍼파라미터 최적화 시작\n",
      "============================================================\n",
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:13:55,067] Trial 0 finished with value: 2.2783424854278564 and parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.00014305097376796446, 'batch_size': 32, 'seq_length': 24, 'optimizer_type': 'Adam', 'weight_decay': 0.0007388012496161985, 'loss_function': 'MSE'}. Best is trial 0 with value: 2.2783424854278564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:13:56,515] Trial 1 finished with value: 1.026620626449585 and parameters: {'hidden_size': 32, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.00025208715781517907, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 0.00020604804798846015, 'loss_function': 'MAE'}. Best is trial 1 with value: 1.026620626449585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 120, Val: 14, Test: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:13:58,785] Trial 2 finished with value: 0.7842395305633545 and parameters: {'hidden_size': 96, 'num_layers': 2, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0042421001265853875, 'batch_size': 16, 'seq_length': 12, 'optimizer_type': 'AdamW', 'weight_decay': 0.0006430862257248823, 'loss_function': 'Huber'}. Best is trial 2 with value: 0.7842395305633545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 108, Val: 12, Test: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:01,930] Trial 3 finished with value: 0.912738561630249 and parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0015300976377717688, 'batch_size': 32, 'seq_length': 30, 'optimizer_type': 'AdamW', 'weight_decay': 4.008958080592501e-05, 'loss_function': 'Huber'}. Best is trial 2 with value: 0.7842395305633545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:02,896] Trial 4 finished with value: inf and parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout_rate': 0.4, 'learning_rate': 0.0011079238637600299, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'Adam', 'weight_decay': 0.00018856802051881794, 'loss_function': 'MSE'}. Best is trial 2 with value: 0.7842395305633545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial failed: \n",
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:03,899] Trial 5 finished with value: 0.6433122158050537 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.0031132944430118203, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 0.00016797361757880054, 'loss_function': 'Huber'}. Best is trial 5 with value: 0.6433122158050537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 116, Val: 13, Test: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:05,124] Trial 6 finished with value: 0.7060281038284302 and parameters: {'hidden_size': 32, 'num_layers': 2, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.00023955348611195168, 'batch_size': 64, 'seq_length': 18, 'optimizer_type': 'Adam', 'weight_decay': 0.00015648038850144677, 'loss_function': 'Huber'}. Best is trial 5 with value: 0.6433122158050537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 116, Val: 13, Test: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:05,819] Trial 7 finished with value: inf and parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.0010669409064523658, 'batch_size': 32, 'seq_length': 18, 'optimizer_type': 'Adam', 'weight_decay': 0.0002806687306867555, 'loss_function': 'MAE'}. Best is trial 5 with value: 0.6433122158050537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial failed: \n",
      "Train: 108, Val: 12, Test: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:06,252] Trial 8 finished with value: inf and parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.0006899455806979155, 'batch_size': 32, 'seq_length': 30, 'optimizer_type': 'Adam', 'weight_decay': 0.0002433855589350274, 'loss_function': 'Huber'}. Best is trial 5 with value: 0.6433122158050537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial failed: \n",
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:06,781] Trial 9 finished with value: inf and parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.0005833352019192215, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'Adam', 'weight_decay': 8.65644341041599e-05, 'loss_function': 'MSE'}. Best is trial 5 with value: 0.6433122158050537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial failed: \n",
      "Train: 120, Val: 14, Test: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:07,063] Trial 10 finished with value: 0.7564995884895325 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.1, 'learning_rate': 0.004937195096294677, 'batch_size': 64, 'seq_length': 12, 'optimizer_type': 'AdamW', 'weight_decay': 1.3246314519833654e-05, 'loss_function': 'Huber'}. Best is trial 5 with value: 0.6433122158050537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 116, Val: 13, Test: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:07,736] Trial 11 finished with value: 0.6822656393051147 and parameters: {'hidden_size': 32, 'num_layers': 1, 'dropout_rate': 0.1, 'learning_rate': 0.0003676987594795991, 'batch_size': 64, 'seq_length': 18, 'optimizer_type': 'AdamW', 'weight_decay': 7.683147488817311e-05, 'loss_function': 'Huber'}. Best is trial 5 with value: 0.6433122158050537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 116, Val: 13, Test: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:08,115] Trial 12 finished with value: 0.6783724427223206 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.1, 'learning_rate': 0.0023981143628413067, 'batch_size': 64, 'seq_length': 18, 'optimizer_type': 'AdamW', 'weight_decay': 5.617846885232925e-05, 'loss_function': 'Huber'}. Best is trial 5 with value: 0.6433122158050537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 116, Val: 13, Test: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:08,528] Trial 13 finished with value: 0.6694490909576416 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.1, 'learning_rate': 0.002483943936548651, 'batch_size': 64, 'seq_length': 18, 'optimizer_type': 'AdamW', 'weight_decay': 3.339916124721981e-05, 'loss_function': 'Huber'}. Best is trial 5 with value: 0.6433122158050537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:09,611] Trial 14 finished with value: 0.6222841739654541 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.002364500113408883, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 2.0985086725403104e-05, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:10,166] Trial 15 finished with value: inf and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.0025750333513609113, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 1.2367314821260336e-05, 'loss_function': 'MAE'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial failed: \n",
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:11,159] Trial 16 finished with value: 0.6257722973823547 and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.0015313636462003012, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 2.3507960386724513e-05, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:12,126] Trial 17 finished with value: 0.6481694579124451 and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.0013125576662534784, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 2.258829942554819e-05, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:12,645] Trial 18 finished with value: inf and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.0018954170054793704, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 2.7046905391181913e-05, 'loss_function': 'MSE'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial failed: \n",
      "Train: 108, Val: 12, Test: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:13,302] Trial 19 finished with value: inf and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0009231119655141637, 'batch_size': 16, 'seq_length': 30, 'optimizer_type': 'AdamW', 'weight_decay': 1.959672207916916e-05, 'loss_function': 'MAE'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial failed: \n",
      "Train: 120, Val: 14, Test: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:14,359] Trial 20 finished with value: 0.7125592231750488 and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout_rate': 0.1, 'learning_rate': 0.0005413588158247894, 'batch_size': 16, 'seq_length': 12, 'optimizer_type': 'AdamW', 'weight_decay': 5.026517909460838e-05, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:15,363] Trial 21 finished with value: 0.664354681968689 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.0033203912078536867, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 1.0304626273174093e-05, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:16,361] Trial 22 finished with value: 0.6502553820610046 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.001884096298418265, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 0.0004027237280270681, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:17,358] Trial 23 finished with value: 0.722137451171875 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.00357709889810728, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 0.00012047777287027026, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:18,347] Trial 24 finished with value: 0.6379360556602478 and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.0016796442892435008, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 1.738438972891986e-05, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:19,376] Trial 25 finished with value: 0.6289108991622925 and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0017070065959049622, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 1.809916214099414e-05, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:20,374] Trial 26 finished with value: 0.6266042590141296 and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0013623485379879788, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 1.582909201882192e-05, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:21,390] Trial 27 finished with value: 0.6581193804740906 and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0008962444444567719, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 3.0096474541209145e-05, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 108, Val: 12, Test: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:21,825] Trial 28 finished with value: inf and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0007704905044372178, 'batch_size': 32, 'seq_length': 30, 'optimizer_type': 'AdamW', 'weight_decay': 5.498944318910848e-05, 'loss_function': 'MAE'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial failed: \n",
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:22,883] Trial 29 finished with value: inf and parameters: {'hidden_size': 96, 'num_layers': 2, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0012898356687162436, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'Adam', 'weight_decay': 1.3148945596714716e-05, 'loss_function': 'MSE'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial failed: \n",
      "\n",
      "Best Hyperparameters:\n",
      "  hidden_size: 128\n",
      "  num_layers: 1\n",
      "  dropout_rate: 0.2\n",
      "  learning_rate: 0.002364500113408883\n",
      "  batch_size: 16\n",
      "  seq_length: 24\n",
      "  optimizer_type: AdamW\n",
      "  weight_decay: 2.0985086725403104e-05\n",
      "  loss_function: Huber\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TASK 5: 하이퍼파라미터 튜닝\n",
    "# ============================================================\n",
    "\n",
    "def objective_improved(trial):\n",
    "    \"\"\"Optuna 목적 함수\"\"\"\n",
    "    params = {\n",
    "        'hidden_size': trial.suggest_categorical('hidden_size', [32, 64, 96, 128]),\n",
    "        'num_layers': trial.suggest_int('num_layers', 1, 2),\n",
    "        'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.4, step=0.1),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 5e-3, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64]),\n",
    "        'seq_length': trial.suggest_categorical('seq_length', [12, 18, 24, 30]),\n",
    "        'optimizer_type': trial.suggest_categorical('optimizer_type', ['Adam', 'AdamW']),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True),\n",
    "        'loss_function': trial.suggest_categorical('loss_function', ['MSE', 'MAE', 'Huber'])\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        train_loader, val_loader, _, _, _ = prepare_data_no_leakage(\n",
    "            X, y, seq_length=params['seq_length'], batch_size=params['batch_size']\n",
    "        )\n",
    "        \n",
    "        model = MultivariateLSTM(\n",
    "            input_size=X.shape[1],\n",
    "            hidden_size=params['hidden_size'],\n",
    "            num_layers=params['num_layers'],\n",
    "            output_size=y.shape[1],\n",
    "            dropout_rate=params['dropout_rate']\n",
    "        ).to(device)\n",
    "        \n",
    "        if params['optimizer_type'] == 'Adam':\n",
    "            optimizer = optim.Adam(model.parameters(), \n",
    "                                  lr=params['learning_rate'],\n",
    "                                  weight_decay=params['weight_decay'])\n",
    "        else:\n",
    "            optimizer = optim.AdamW(model.parameters(), \n",
    "                                   lr=params['learning_rate'],\n",
    "                                   weight_decay=params['weight_decay'])\n",
    "        \n",
    "        if params['loss_function'] == 'MSE':\n",
    "            criterion = nn.MSELoss()\n",
    "        elif params['loss_function'] == 'MAE':\n",
    "            criterion = nn.L1Loss()\n",
    "        else:\n",
    "            criterion = nn.HuberLoss(delta=1.0)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience = 0\n",
    "        max_patience = 10\n",
    "        \n",
    "        for epoch in range(30):\n",
    "            model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in val_loader:\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= max_patience:\n",
    "                    break\n",
    "            \n",
    "            trial.report(avg_val_loss, epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        return best_val_loss\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed: {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"하이퍼파라미터 최적화 시작\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=5)\n",
    ")\n",
    "\n",
    "study.optimize(objective_improved, n_trials=30, timeout=600)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "best_params = best_trial.params\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5771e00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "최적 모델 학습\n",
      "============================================================\n",
      "Train: 111, Val: 13, Test: 32\n",
      "Epoch   1: Train=0.332202, Val=0.632663\n",
      "Epoch   1: Train=0.332202, Val=0.632663\n",
      "Epoch  11: Train=0.188548, Val=1.304677 (10/20)\n",
      "Epoch  11: Train=0.188548, Val=1.304677 (10/20)\n",
      "Epoch  21: Train=0.122801, Val=1.223255 (20/20)\n",
      "Early stopping at epoch 21\n",
      "Best model restored with validation loss: 0.632663\n",
      "Epoch  21: Train=0.122801, Val=1.223255 (20/20)\n",
      "Early stopping at epoch 21\n",
      "Best model restored with validation loss: 0.632663\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TASK 6: 최적 모델 학습\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"최적 모델 학습\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 데이터 준비\n",
    "train_loader, val_loader, test_loader, scaler_X, scaler_y = prepare_data_no_leakage(\n",
    "    X, y, seq_length=best_params['seq_length'], batch_size=best_params['batch_size']\n",
    ")\n",
    "\n",
    "# 최적 모델 생성\n",
    "final_model = MultivariateLSTM(\n",
    "    input_size=X.shape[1],\n",
    "    hidden_size=best_params['hidden_size'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    output_size=y.shape[1],\n",
    "    dropout_rate=best_params['dropout_rate']\n",
    ").to(device)\n",
    "\n",
    "# 옵티마이저 및 손실함수 설정\n",
    "if best_params['optimizer_type'] == 'Adam':\n",
    "    optimizer = optim.Adam(final_model.parameters(), \n",
    "                          lr=best_params['learning_rate'],\n",
    "                          weight_decay=best_params['weight_decay'])\n",
    "else:\n",
    "    optimizer = optim.AdamW(final_model.parameters(), \n",
    "                           lr=best_params['learning_rate'],\n",
    "                           weight_decay=best_params['weight_decay'])\n",
    "\n",
    "if best_params['loss_function'] == 'MSE':\n",
    "    criterion = nn.MSELoss()\n",
    "elif best_params['loss_function'] == 'MAE':\n",
    "    criterion = nn.L1Loss()\n",
    "else:\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "# 모델 학습\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "max_patience = 20\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(200):\n",
    "    # Training\n",
    "    final_model.train()\n",
    "    train_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = final_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(final_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    final_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            outputs = final_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = final_model.state_dict().copy()\n",
    "        print(f\"Epoch {epoch+1:3d}: Train={avg_train_loss:.6f}, Val={avg_val_loss:.6f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if epoch % 10 == 0 or patience_counter >= max_patience:\n",
    "            print(f\"Epoch {epoch+1:3d}: Train={avg_train_loss:.6f}, Val={avg_val_loss:.6f} ({patience_counter}/{max_patience})\")\n",
    "    \n",
    "    if patience_counter >= max_patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# 최적 가중치 복원\n",
    "if best_model_state is not None:\n",
    "    final_model.load_state_dict(best_model_state)\n",
    "    print(f\"Best model restored with validation loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "170d9c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "모델 평가\n",
      "============================================================\n",
      "construction_bsi_actual: RMSE=3.0729, MAE=2.6360\n",
      "base_rate: RMSE=0.1245, MAE=0.0833\n",
      "housing_sale_price: RMSE=0.3289, MAE=0.1983\n",
      "m2_growth: RMSE=0.3868, MAE=0.2829\n",
      "credit_spread: RMSE=0.0851, MAE=0.0587\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TASK 7: 모델 평가\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"모델 평가\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "final_model.eval()\n",
    "test_predictions = []\n",
    "test_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = final_model(batch_X)\n",
    "        test_predictions.append(outputs.cpu().numpy())\n",
    "        test_targets.append(batch_y.cpu().numpy())\n",
    "\n",
    "test_predictions = np.vstack(test_predictions)\n",
    "test_targets = np.vstack(test_targets)\n",
    "\n",
    "# 역정규화\n",
    "test_predictions_diff = scaler_y.inverse_transform(test_predictions)\n",
    "test_targets_diff = scaler_y.inverse_transform(test_targets)\n",
    "\n",
    "# 성능 메트릭 계산\n",
    "test_metrics = {}\n",
    "for i, target in enumerate(available_targets):\n",
    "    if i < test_predictions_diff.shape[1]:\n",
    "        rmse = np.sqrt(mean_squared_error(test_targets_diff[:, i], test_predictions_diff[:, i]))\n",
    "        mae = mean_absolute_error(test_targets_diff[:, i], test_predictions_diff[:, i])\n",
    "        \n",
    "        test_metrics[target] = {'RMSE': rmse, 'MAE': mae}\n",
    "        print(f\"{target}: RMSE={rmse:.4f}, MAE={mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a0f182c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "재귀적 미래 예측 (3개월)\n",
      "============================================================\n",
      "\n",
      "=== Month 1 Prediction ===\n",
      "Predicted diff: [-1.7079172e+00 -1.5892196e-03  2.4743013e-01 -5.3899044e-01\n",
      "  1.6328173e-02]\n",
      "Predicted values: [67.29208279  2.49841078 93.60443013  0.46100956  5.85432817]\n",
      "Updated DataFrame shape: (181, 77)\n",
      "\n",
      "=== Month 2 Prediction ===\n",
      "Predicted diff: [-0.6047344  -0.02191541  0.29684326 -0.16727237  0.01166489]\n",
      "Predicted values: [66.68734837  2.47649537 93.90127339  0.29373719  5.86599306]\n",
      "Updated DataFrame shape: (182, 77)\n",
      "\n",
      "=== Month 3 Prediction ===\n",
      "Predicted diff: [ 0.35494673 -0.01862453  0.31391495  0.13839273  0.01259501]\n",
      "Predicted values: [67.0422951   2.45787084 94.21518835  0.43212992  5.87858807]\n",
      "Updated DataFrame shape: (183, 77)\n",
      "\n",
      "미래 3개월 예측 결과:\n",
      "            construction_bsi_actual  base_rate  housing_sale_price  m2_growth  \\\n",
      "2025-09-01                67.292083   2.498411           93.604430   0.461010   \n",
      "2025-10-01                66.687348   2.476495           93.901273   0.293737   \n",
      "2025-11-01                67.042295   2.457871           94.215188   0.432130   \n",
      "\n",
      "            credit_spread  \n",
      "2025-09-01       5.854328  \n",
      "2025-10-01       5.865993  \n",
      "2025-11-01       5.878588  \n",
      "\n",
      "최근 실제값 (비교용):\n",
      "            construction_bsi_actual  base_rate  housing_sale_price  m2_growth  \\\n",
      "date                                                                            \n",
      "2025-06-01                     68.0        2.5              93.164       0.63   \n",
      "2025-07-01                     68.0        2.5              93.317       1.00   \n",
      "2025-08-01                     69.0        2.5              93.357       1.00   \n",
      "\n",
      "            credit_spread  \n",
      "date                       \n",
      "2025-06-01          5.783  \n",
      "2025-07-01          5.827  \n",
      "2025-08-01          5.838  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TASK 8: 재귀적 미래 예측 (3개월)\n",
    "# ============================================================\n",
    "\n",
    "def predict_future_recursive(model, df_original, scaler_X, scaler_y, final_features, seq_length, n_months=3):\n",
    "    \"\"\"재귀적 미래 예측 - 각 단계의 예측값으로 다음 피쳐를 업데이트\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 현재 데이터 복사\n",
    "    df_pred = df_original.copy()\n",
    "    future_predictions = []\n",
    "    \n",
    "    for month in range(n_months):\n",
    "        print(f\"\\n=== Month {month+1} Prediction ===\")\n",
    "        \n",
    "        # 현재 시점의 마지막 시퀀스 추출\n",
    "        current_features = df_pred[final_features].iloc[-seq_length:].values\n",
    "        current_features_scaled = scaler_X.transform(current_features)\n",
    "        \n",
    "        # 예측 수행\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(current_features_scaled).unsqueeze(0).to(device)\n",
    "            pred_scaled = model(X_tensor).cpu().numpy()\n",
    "            pred_diff = scaler_y.inverse_transform(pred_scaled)[0]\n",
    "        \n",
    "        # 차분값을 원시값으로 변환\n",
    "        last_original_values = df_pred[available_targets].iloc[-1].values\n",
    "        predicted_values = last_original_values + pred_diff\n",
    "        \n",
    "        future_predictions.append(predicted_values.copy())\n",
    "        print(f\"Predicted diff: {pred_diff}\")\n",
    "        print(f\"Predicted values: {predicted_values}\")\n",
    "        \n",
    "        # 다음 예측을 위해 DataFrame 업데이트\n",
    "        next_date = df_pred.index[-1] + pd.DateOffset(months=1)\n",
    "        \n",
    "        # 새로운 행 생성 (기본값으로 초기화)\n",
    "        new_row = df_pred.iloc[-1].copy()\n",
    "        \n",
    "        # 타겟 변수 업데이트\n",
    "        for i, target in enumerate(available_targets):\n",
    "            new_row[target] = predicted_values[i]\n",
    "            new_row[f'{target}_diff'] = pred_diff[i]\n",
    "        \n",
    "        # 지연 변수 업데이트 (lag features)\n",
    "        for target in available_targets:\n",
    "            if f'{target}_lag1' in new_row.index:\n",
    "                new_row[f'{target}_lag1'] = df_pred[target].iloc[-1]\n",
    "            if f'{target}_lag3' in new_row.index:\n",
    "                new_row[f'{target}_lag3'] = df_pred[target].iloc[-3] if len(df_pred) >= 3 else df_pred[target].iloc[0]\n",
    "            if f'{target}_lag6' in new_row.index:\n",
    "                new_row[f'{target}_lag6'] = df_pred[target].iloc[-6] if len(df_pred) >= 6 else df_pred[target].iloc[0]\n",
    "        \n",
    "        # 이동평균 업데이트 (간단화된 버전)\n",
    "        for target in available_targets:\n",
    "            diff_col = f'{target}_diff'\n",
    "            if f'{diff_col}_ma3' in new_row.index:\n",
    "                recent_diffs = df_pred[diff_col].iloc[-2:].tolist() + [pred_diff[available_targets.index(target)]]\n",
    "                new_row[f'{diff_col}_ma3'] = np.mean(recent_diffs)\n",
    "            if f'{diff_col}_ma6' in new_row.index:\n",
    "                recent_diffs = df_pred[diff_col].iloc[-5:].tolist() + [pred_diff[available_targets.index(target)]]\n",
    "                new_row[f'{diff_col}_ma6'] = np.mean(recent_diffs)\n",
    "        \n",
    "        # DataFrame에 새로운 행 추가\n",
    "        new_row.name = next_date\n",
    "        df_pred = pd.concat([df_pred, new_row.to_frame().T])\n",
    "        \n",
    "        print(f\"Updated DataFrame shape: {df_pred.shape}\")\n",
    "    \n",
    "    return np.array(future_predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"재귀적 미래 예측 (3개월)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 재귀적 예측 수행\n",
    "future_predictions_recursive = predict_future_recursive(\n",
    "    final_model, df, scaler_X, scaler_y, final_features, \n",
    "    best_params['seq_length'], n_months=3\n",
    ")\n",
    "\n",
    "# 예측 결과를 DataFrame으로 변환\n",
    "last_date = df.index[-1]\n",
    "future_dates = pd.date_range(start=last_date + pd.DateOffset(months=1), periods=3, freq='MS')\n",
    "\n",
    "predictions_df = pd.DataFrame(\n",
    "    future_predictions_recursive,\n",
    "    columns=available_targets,\n",
    "    index=future_dates\n",
    ")\n",
    "\n",
    "print(\"\\n미래 3개월 예측 결과:\")\n",
    "print(predictions_df)\n",
    "\n",
    "# 최근 실제값과 비교\n",
    "print(\"\\n최근 실제값 (비교용):\")\n",
    "recent_values = df[available_targets].tail(3)\n",
    "print(recent_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b625728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "예측 결과 DB 저장\n",
      "============================================================\n",
      "모델 예측 결과 3건 저장 완료\n",
      "예측 결과 DB 저장 완료\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TASK 9: 예측 결과 DB 저장\n",
    "# ============================================================\n",
    "\n",
    "def save_predictions_to_db(predictions_df, db_conn):\n",
    "    \"\"\"예측 결과를 model_output 테이블에 저장\"\"\"\n",
    "    if not db_conn:\n",
    "        print(\"DB 연결 없음 - 파일로만 저장\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # 데이터 준비 (DDL 스키마에 맞춤)\n",
    "        pred_save = predictions_df.copy()\n",
    "        pred_save.reset_index(inplace=True)\n",
    "        \n",
    "        # 인덱스 컬럼명을 'date'로 변경\n",
    "        if 'index' in pred_save.columns:\n",
    "            pred_save.rename(columns={'index': 'date'}, inplace=True)\n",
    "        \n",
    "        pred_save['date'] = pd.to_datetime(pred_save['date']).dt.strftime('%Y%m')\n",
    "        \n",
    "        # credit_spread 컬럼 추가 (없는 경우)\n",
    "        if 'credit_spread' not in pred_save.columns:\n",
    "            pred_save['credit_spread'] = 0.0\n",
    "        \n",
    "        success = db_conn.save_model_output(pred_save)\n",
    "        return success\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"예측 결과 저장 실패: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"예측 결과 DB 저장\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pred_save_success = save_predictions_to_db(predictions_df, db_connection)\n",
    "if pred_save_success:\n",
    "    print(\"예측 결과 DB 저장 완료\")\n",
    "else:\n",
    "    print(\"예측 결과 저장 실패\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d499a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "모델 아티팩트 저장\n",
      "============================================================\n",
      "모델 아티팩트 저장: lstm_model_artifacts.pkl\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TASK 10: 모델 아티팩트 저장\n",
    "# ============================================================\n",
    "\n",
    "def save_model_artifacts(artifacts, filepath='output/lstm_model_artifacts.pkl'):\n",
    "    \"\"\"모델 아티팩트를 파일로 저장\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(artifacts, f)\n",
    "        print(f\"모델 아티팩트 저장: {filepath}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"아티팩트 저장 실패: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"모델 아티팩트 저장\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 베이스라인 메트릭 (간단한 버전)\n",
    "baseline_metrics = {}\n",
    "for target in available_targets:\n",
    "    naive_pred = np.full(len(test_targets_diff), df[target].iloc[-1])\n",
    "    if len(test_targets_diff) > 0:\n",
    "        target_idx = available_targets.index(target)\n",
    "        rmse = np.sqrt(mean_squared_error(test_targets_diff[:, target_idx], naive_pred))\n",
    "        baseline_metrics[f'{target}_naive_rmse'] = rmse\n",
    "\n",
    "# 아티팩트 구성\n",
    "artifacts = {\n",
    "    \"model_state_dict\": {k: v.cpu() for k, v in final_model.state_dict().items()},\n",
    "    \"hyperparameters\": best_params,\n",
    "    \"scaler_X\": scaler_X,\n",
    "    \"scaler_y\": scaler_y,\n",
    "    \"final_features\": final_features,\n",
    "    \"target_columns\": available_targets,\n",
    "    \"test_metrics\": test_metrics,\n",
    "    \"baseline_metrics\": baseline_metrics,\n",
    "    \"training_losses\": train_losses,\n",
    "    \"validation_losses\": val_losses\n",
    "}\n",
    "\n",
    "artifact_saved = save_model_artifacts(artifacts, os.path.join('output', 'lstm_model_artifacts.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d3285",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3439066478.py, line 86)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mplt.title(f'{target}: Historical vs Future Predictions')plt.show()\u001b[39m\n                                                            ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TASK 11: 결과 파일 저장 및 시각화\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"결과 파일 저장\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# output 폴더 생성\n",
    "import os\n",
    "output_dir = 'output'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"✓ {output_dir} 폴더 생성\")\n",
    "\n",
    "try:\n",
    "    # 예측 결과 저장\n",
    "    predictions_df.to_csv(os.path.join(output_dir, 'lstm_predictions_3months.csv'))\n",
    "    print(f\"예측 결과 저장: {output_dir}/lstm_predictions_3months.csv\")\n",
    "    \n",
    "    # 성능 메트릭 저장\n",
    "    metrics_df = pd.DataFrame(test_metrics).T\n",
    "    metrics_df.to_csv(os.path.join(output_dir, 'lstm_performance_metrics.csv'))\n",
    "    print(f\"성능 메트릭 저장: {output_dir}/lstm_performance_metrics.csv\")\n",
    "    \n",
    "    # 모델 체크포인트 저장\n",
    "    torch.save({\n",
    "        'model_state_dict': final_model.state_dict(),\n",
    "        'hyperparameters': best_params,\n",
    "        'scaler_X': scaler_X,\n",
    "        'scaler_y': scaler_y\n",
    "    }, os.path.join(output_dir, 'lstm_model_checkpoint.pth'))\n",
    "    print(f\"모델 체크포인트 저장: {output_dir}/lstm_model_checkpoint.pth\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"파일 저장 오류: {e}\")\n",
    "\n",
    "# 간단한 시각화\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"시각화\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 학습 곡선\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training History')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 예측 vs 실제 (첫 번째 타겟만)\n",
    "plt.subplot(1, 2, 2)\n",
    "if len(test_predictions_diff) > 0 and len(available_targets) > 0:\n",
    "    target_idx = 0\n",
    "    plt.scatter(test_targets_diff[:, target_idx], test_predictions_diff[:, target_idx], alpha=0.6)\n",
    "    min_val = min(test_targets_diff[:, target_idx].min(), test_predictions_diff[:, target_idx].min())\n",
    "    max_val = max(test_targets_diff[:, target_idx].max(), test_predictions_diff[:, target_idx].max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title(f'{available_targets[0]} Prediction')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 미래 예측 시각화\n",
    "plt.figure(figsize=(15, 3*len(available_targets)))\n",
    "for i, target in enumerate(available_targets):\n",
    "    plt.subplot(len(available_targets), 1, i+1)\n",
    "    \n",
    "    # 최근 24개월 실제 데이터\n",
    "    recent_data = df[target].tail(24)\n",
    "    future_data = predictions_df[target]\n",
    "    \n",
    "    plt.plot(recent_data.index, recent_data.values, 'o-', label='Historical', alpha=0.7)\n",
    "    plt.plot(future_data.index, future_data.values, 's-', label='Predicted', color='red', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target)\n",
    "    plt.title(f'{target}: Historical vs Future Predictions')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cb41cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LSTM 모델링 파이프라인 완료\n",
      "============================================================\n",
      "생성된 파일:\n",
      "  • output/lstm_predictions_3months.csv\n",
      "  • output/lstm_performance_metrics.csv\n",
      "  • output/lstm_model_checkpoint.pth\n",
      "  • output/lstm_model_artifacts.pkl\n",
      "\n",
      "최종 성능:\n",
      "  construction_bsi_actual: RMSE = 3.0729, MAE = 2.6360\n",
      "  base_rate: RMSE = 0.1245, MAE = 0.0833\n",
      "  housing_sale_price: RMSE = 0.3289, MAE = 0.1983\n",
      "  m2_growth: RMSE = 0.3868, MAE = 0.2829\n",
      "  credit_spread: RMSE = 0.0851, MAE = 0.0587\n",
      "\n",
      "예측 결과 요약:\n",
      "  construction_bsi_actual: 현재 69.000 → 3개월 후 평균 67.007 (변화: -1.993)\n",
      "  base_rate: 현재 2.500 → 3개월 후 평균 2.478 (변화: -0.022)\n",
      "  housing_sale_price: 현재 93.357 → 3개월 후 평균 93.907 (변화: +0.550)\n",
      "  m2_growth: 현재 1.000 → 3개월 후 평균 0.396 (변화: -0.604)\n",
      "  credit_spread: 현재 5.838 → 3개월 후 평균 5.866 (변화: +0.028)\n",
      "MySQL 연결 해제\n",
      "\n",
      "DB 연결 종료 완료\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TASK 12: 최종 정리 및 DB 연결 종료\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LSTM 모델링 파이프라인 완료\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"생성된 파일:\")\n",
    "print(\"  • output/lstm_predictions_3months.csv\")\n",
    "print(\"  • output/lstm_performance_metrics.csv\") \n",
    "print(\"  • output/lstm_model_checkpoint.pth\")\n",
    "print(\"  • output/lstm_model_artifacts.pkl\")\n",
    "\n",
    "print(f\"\\n최종 성능:\")\n",
    "for target, metrics in test_metrics.items():\n",
    "    print(f\"  {target}: RMSE = {metrics['RMSE']:.4f}, MAE = {metrics['MAE']:.4f}\")\n",
    "\n",
    "print(f\"\\n예측 결과 요약:\")\n",
    "for i, target in enumerate(available_targets):\n",
    "    last_actual = df[target].iloc[-1]\n",
    "    pred_values = predictions_df[target].values\n",
    "    avg_change = np.mean(pred_values - last_actual)\n",
    "    print(f\"  {target}: 현재 {last_actual:.3f} → 3개월 후 평균 {np.mean(pred_values):.3f} (변화: {avg_change:+.3f})\")\n",
    "\n",
    "# DB 연결 정리\n",
    "if db_connection:\n",
    "    db_connection.disconnect()\n",
    "    print(\"\\nDB 연결 종료 완료\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
