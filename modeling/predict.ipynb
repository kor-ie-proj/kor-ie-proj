{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9da9aea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "IE_project Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÑ§Ï†ï\n",
      "============================================================\n",
      "‚úì IE_project Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÉùÏÑ±/ÏÑ†ÌÉù ÏôÑÎ£å\n",
      "‚úì model_output ÌÖåÏù¥Î∏î ÏÉùÏÑ±/ÌôïÏù∏ ÏôÑÎ£å\n",
      "‚úì Ï¥ù ÌÖåÏù¥Î∏î Ïàò: 4\n",
      "ÌÖåÏù¥Î∏î Î™©Î°ù: ['dart_data', 'ecos_data', 'final_features', 'model_output']\n",
      "‚úì IE_project Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ ÌÖåÏä§Ìä∏ ÏÑ±Í≥µ!\n",
      "\n",
      "üéâ IE_project Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÑ§Ï†ï ÏôÑÎ£å!\n",
      "Ïù¥Ï†ú predict.ipynbÏóêÏÑú IE_project DBÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n"
     ]
    }
   ],
   "source": [
    "# IE_project Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÇ¨Ïö© Î∞è model_output ÌÖåÏù¥Î∏î ÏÉùÏÑ±\n",
    "import os\n",
    "import sys\n",
    "import mysql.connector\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# DB Ìè¥ÎçîÏùò .env ÌååÏùº Î°úÎìú\n",
    "db_folder = os.path.join(os.path.dirname(os.getcwd()), 'DB')\n",
    "env_path = os.path.join(db_folder, '.env')\n",
    "load_dotenv(env_path)\n",
    "\n",
    "def setup_ie_project_db():\n",
    "    \"\"\"IE_project Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÑ§Ï†ï Î∞è ÌÖåÏù¥Î∏î ÏÉùÏÑ±\"\"\"\n",
    "    try:\n",
    "        # Î®ºÏ†Ä Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏóÜÏù¥ Ïó∞Í≤∞\n",
    "        config_no_db = {\n",
    "            'host': 'localhost',\n",
    "            'user': 'root',\n",
    "            'password': 'Woorifisa5!',\n",
    "            'charset': 'utf8mb4'\n",
    "        }\n",
    "        \n",
    "        connection = mysql.connector.connect(**config_no_db)\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # IE_project Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÉùÏÑ± Î∞è ÏÇ¨Ïö©\n",
    "        cursor.execute(\"CREATE DATABASE IF NOT EXISTS IE_project\")\n",
    "        cursor.execute(\"USE IE_project\")\n",
    "        print(\"‚úì IE_project Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÉùÏÑ±/ÏÑ†ÌÉù ÏôÑÎ£å\")\n",
    "        \n",
    "        # model_output ÌÖåÏù¥Î∏î ÏÉùÏÑ± (ÎàÑÎùΩÎêú ÌÖåÏù¥Î∏î)\n",
    "        model_output_ddl = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS model_output (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            date VARCHAR(6) NOT NULL,\n",
    "            construction_bsi_actual DECIMAL(5, 1),\n",
    "            base_rate DECIMAL(5, 3),\n",
    "            housing_sale_price DECIMAL(8, 3),\n",
    "            m2_growth DECIMAL(8, 2),\n",
    "            credit_spread DECIMAL(8, 3),\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n",
    "            UNIQUE KEY unique_date (date)\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(model_output_ddl)\n",
    "        print(\"‚úì model_output ÌÖåÏù¥Î∏î ÏÉùÏÑ±/ÌôïÏù∏ ÏôÑÎ£å\")\n",
    "        \n",
    "        # ÌÖåÏù¥Î∏î Î™©Î°ù ÌôïÏù∏\n",
    "        cursor.execute(\"SHOW TABLES\")\n",
    "        tables = cursor.fetchall()\n",
    "        table_names = [table[0] for table in tables]\n",
    "        print(f\"‚úì Ï¥ù ÌÖåÏù¥Î∏î Ïàò: {len(tables)}\")\n",
    "        print(f\"ÌÖåÏù¥Î∏î Î™©Î°ù: {table_names}\")\n",
    "        \n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        \n",
    "        # IE_project Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Î°ú Ïó∞Í≤∞ ÌÖåÏä§Ìä∏\n",
    "        config_with_db = {\n",
    "            'host': 'localhost',\n",
    "            'user': 'root',\n",
    "            'password': 'Woorifisa5!',\n",
    "            'database': 'IE_project',\n",
    "            'charset': 'utf8mb4'\n",
    "        }\n",
    "        \n",
    "        connection = mysql.connector.connect(**config_with_db)\n",
    "        print(\"‚úì IE_project Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ ÌÖåÏä§Ìä∏ ÏÑ±Í≥µ!\")\n",
    "        connection.close()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÑ§Ï†ï Ïò§Î•ò: {e}\")\n",
    "        return False\n",
    "\n",
    "# IE_project Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÑ§Ï†ï\n",
    "print(\"=\"*60)\n",
    "print(\"IE_project Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÑ§Ï†ï\")\n",
    "print(\"=\"*60)\n",
    "success = setup_ie_project_db()\n",
    "if success:\n",
    "    print(\"\\nüéâ IE_project Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÑ§Ï†ï ÏôÑÎ£å!\")\n",
    "    print(\"Ïù¥Ï†ú predict.ipynbÏóêÏÑú IE_project DBÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÑ§Ï†ï Ïã§Ìå®!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3ecf7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TASK 1: ÎùºÏù¥Î∏åÎü¨Î¶¨ import Î∞è ÌôòÍ≤Ω ÏÑ§Ï†ï\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU ÏÑ§Ï†ï\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac31ac8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (187, 28)\n",
      "Available target columns: ['construction_bsi_actual', 'base_rate', 'housing_sale_price', 'm2_growth', 'credit_spread']\n",
      "Original features: 67\n",
      "Final selected features: 26\n",
      "Features shape: (180, 26)\n",
      "Targets shape: (180, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# TASK 2: Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è ÌîºÏ≥ê ÏóîÏßÄÎãàÏñ¥ÎßÅ\n",
    "# ============================================================\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "df = pd.read_csv('ecos_monthly_data.csv')\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "\n",
    "# ÎÇ†Ïßú Ïª¨Îüº Ï≤òÎ¶¨\n",
    "if 'date' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').set_index('date')\n",
    "elif 'Date' in df.columns:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values('Date').set_index('Date')\n",
    "\n",
    "# ÌÉÄÍ≤ü Î≥ÄÏàò Ï†ïÏùò\n",
    "target_columns = [\n",
    "    'construction_bsi_actual',\n",
    "    'base_rate', \n",
    "    'housing_sale_price',\n",
    "    'm2_growth',\n",
    "    'credit_spread'\n",
    "]\n",
    "\n",
    "available_targets = [col for col in target_columns if col in df.columns]\n",
    "print(f\"Available target columns: {available_targets}\")\n",
    "\n",
    "# Í≤∞Ï∏°Ïπò Ï≤òÎ¶¨\n",
    "df = df.interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "# ÌÉÄÍ≤ü Î≥ÄÏàò Ï∞®Î∂Ñ Ï†ÅÏö© (ÎπÑÏ†ïÏÉÅÏÑ± Ï†úÍ±∞)\n",
    "for col in available_targets:\n",
    "    df[f'{col}_diff'] = df[col].diff()\n",
    "\n",
    "diff_targets = [f'{col}_diff' for col in available_targets]\n",
    "\n",
    "# ÌîºÏ≥ê ÏóîÏßÄÎãàÏñ¥ÎßÅ: Ïù¥ÎèôÌèâÍ∑†, ÏßÄÏó∞, Î≥ÄÌôîÏú®\n",
    "for col in available_targets:\n",
    "    diff_col = f'{col}_diff'\n",
    "    \n",
    "    # Ïù¥ÎèôÌèâÍ∑†\n",
    "    df[f'{diff_col}_ma3'] = df[diff_col].rolling(window=3, min_periods=1).mean()\n",
    "    df[f'{diff_col}_ma6'] = df[diff_col].rolling(window=6, min_periods=1).mean()\n",
    "    \n",
    "    # Î≥ÄÌôîÏú®\n",
    "    df[f'{diff_col}_pct_change'] = df[diff_col].pct_change().fillna(0)\n",
    "    \n",
    "    # ÏßÄÏó∞ ÌäπÏßï\n",
    "    for lag in [1, 3, 6]:\n",
    "        df[f'{diff_col}_lag{lag}'] = df[diff_col].shift(lag)\n",
    "        df[f'{col}_lag{lag}'] = df[col].shift(lag)\n",
    "\n",
    "# Í≤∞Ï∏°Ïπò Ï†úÍ±∞\n",
    "df = df.dropna()\n",
    "\n",
    "# ÌîºÏ≥ê ÏÑ†ÌÉù: ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Í∏∞Î∞ò\n",
    "all_features = [col for col in df.columns if col not in available_targets and col not in diff_targets]\n",
    "\n",
    "# ÎÜíÏùÄ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÌîºÏ≥ê Ï†úÍ±∞\n",
    "def remove_highly_correlated_features(corr_matrix, threshold=0.95):\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper_tri.columns if any(abs(upper_tri[column]) > threshold)]\n",
    "    return to_drop\n",
    "\n",
    "correlation_matrix = df[all_features].corr()\n",
    "highly_corr_features = remove_highly_correlated_features(correlation_matrix)\n",
    "selected_features = [col for col in all_features if col not in highly_corr_features]\n",
    "\n",
    "# ÌÉÄÍ≤üÍ≥ºÏùò ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Í∏∞Î∞ò ÏµúÏ¢Ö ÌîºÏ≥ê ÏÑ†ÌÉù\n",
    "target_correlations = []\n",
    "for target in diff_targets:\n",
    "    if target in df.columns:\n",
    "        corr_with_target = df[selected_features + [target]].corr()[target].abs().sort_values(ascending=False)\n",
    "        target_correlations.append(corr_with_target[:-1])\n",
    "\n",
    "avg_correlation = pd.concat(target_correlations, axis=1).mean(axis=1).sort_values(ascending=False)\n",
    "n_features = max(20, int(len(selected_features) * 0.5))\n",
    "final_features = avg_correlation.head(n_features).index.tolist()\n",
    "\n",
    "print(f\"Original features: {len(all_features)}\")\n",
    "print(f\"Final selected features: {len(final_features)}\")\n",
    "\n",
    "# ÏµúÏ¢Ö Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ\n",
    "X = df[final_features].values\n",
    "y = df[diff_targets].values\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Targets shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea0e3c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB Î™®Îìà Î°úÎìú ÏÑ±Í≥µ: c:\\Users\\baesh\\Desktop\\kor-ie-proj\\DB\n",
      "DB ÏÑ§Ï†ï Î°úÎìú: c:\\Users\\baesh\\Desktop\\kor-ie-proj\\DB\\.env\n",
      "MySQL Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ ÏÑ±Í≥µ\n",
      "‚úì Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ ÏÑ±Í≥µ!\n",
      "\n",
      "============================================================\n",
      "ÏµúÏ¢Ö ÌîºÏ≥ê DB Ï†ÄÏû•\n",
      "============================================================\n",
      "ÌîºÏ≥ê Îç∞Ïù¥ÌÑ∞ 180Í±¥ Ï†ÄÏû• ÏôÑÎ£å\n",
      "DB Ï†ÄÏû• ÏôÑÎ£å\n",
      "ÌîºÏ≥ê Îç∞Ïù¥ÌÑ∞ 180Í±¥ Ï†ÄÏû• ÏôÑÎ£å\n",
      "DB Ï†ÄÏû• ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TASK 3: Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ Î∞è ÌîºÏ≥ê Ï†ÄÏû•\n",
    "# ============================================================\n",
    "\n",
    "# DB Î™®Îìà import\n",
    "try:\n",
    "    from db_query import DatabaseConnection\n",
    "except ImportError:\n",
    "    # ÎÖ∏Ìä∏Î∂ÅÏóêÏÑúÎäî ÏÉÅÎåÄ Í≤ΩÎ°úÎ•º ÏßÅÏ†ë ÏßÄÏ†ï\n",
    "    import os\n",
    "    import sys\n",
    "    \n",
    "    # modeling Ìè¥ÎçîÏóêÏÑú ÏÉÅÏúÑ Ìè¥ÎçîÏùò DB Ìè¥ÎçîÎ°ú Ïù¥Îèô\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "    db_path = os.path.join(parent_dir, 'DB')\n",
    "    \n",
    "    if db_path not in sys.path:\n",
    "        sys.path.insert(0, db_path)\n",
    "    \n",
    "    try:\n",
    "        from db_query import DatabaseConnection\n",
    "        print(f\"DB Î™®Îìà Î°úÎìú ÏÑ±Í≥µ: {db_path}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"DB Î™®Îìà Î°úÎìú Ïã§Ìå®: {e}\")\n",
    "        DatabaseConnection = None\n",
    "\n",
    "# DB Ìè¥ÎçîÏùò .env ÌååÏùº Î°úÎìú\n",
    "from dotenv import load_dotenv\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "db_folder = os.path.join(parent_dir, 'DB')\n",
    "env_path = os.path.join(db_folder, '.env')\n",
    "load_dotenv(env_path)\n",
    "print(f\"DB ÏÑ§Ï†ï Î°úÎìú: {env_path}\")\n",
    "\n",
    "# DB Ïó∞Í≤∞ ÏÉùÏÑ±\n",
    "db_connection = None\n",
    "if DatabaseConnection:\n",
    "    try:\n",
    "        # DatabaseConnection ÌÅ¥ÎûòÏä§ Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± (DB Ìè¥ÎçîÏùò .env ÏÇ¨Ïö©)\n",
    "        db_connection = DatabaseConnection()\n",
    "        if db_connection.connect():\n",
    "            print(\"‚úì Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ ÏÑ±Í≥µ!\")\n",
    "        else:\n",
    "            print(\"‚úó Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ Ïã§Ìå®\")\n",
    "            db_connection = None\n",
    "    except Exception as e:\n",
    "        print(f\"DB Ïó∞Í≤∞ Ïò§Î•ò: {e}\")\n",
    "        db_connection = None\n",
    "\n",
    "# ÏµúÏ¢Ö ÌîºÏ≥êÎ•º DBÏóê Ï†ÄÏû•\n",
    "def save_features_to_db(df_data, db_conn):\n",
    "    \"\"\"ÏµúÏ¢Ö ÌîºÏ≥êÎ•º final_features ÌÖåÏù¥Î∏îÏóê Ï†ÄÏû•\"\"\"\n",
    "    if not db_conn:\n",
    "        print(\"DB Ïó∞Í≤∞ ÏóÜÏùå - ÌååÏùºÎ°úÎßå Ï†ÄÏû•\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ\n",
    "        df_save = df_data.copy()\n",
    "        df_save.reset_index(inplace=True)\n",
    "        df_save['date'] = pd.to_datetime(df_save['date']).dt.strftime('%Y%m')\n",
    "        \n",
    "        # DDL Ïä§ÌÇ§ÎßàÏóê ÎßûÎäî Ïª¨ÎüºÎßå ÏÑ†ÌÉù\n",
    "        schema_columns = [\n",
    "            'date', 'construction_bsi_actual_diff', 'housing_sale_price_diff', \n",
    "            'm2_growth_diff', 'credit_spread_diff', 'base_rate_diff',\n",
    "            'construction_bsi_mom', 'housing_sale_price_diff_ma3', 'm2_growth_lag1',\n",
    "            'base_rate_mdiff_bp', 'credit_spread_diff_ma3', 'construction_bsi_ma3',\n",
    "            'leading_index', 'housing_sale_price_diff_lag6', 'construction_bsi_actual_lag3',\n",
    "            'construction_bsi_actual_diff_ma3', 'base_rate_diff_ma6', 'term_spread',\n",
    "            'construction_bsi_actual_diff_ma6', 'credit_spread_diff_lag1',\n",
    "            'market_rate_treasury_bond_3yr', 'credit_spread_diff_ma6', 'base_rate_diff_ma3',\n",
    "            'base_rate_lag1', 'esi', 'base_rate_diff_lag3', 'm2_growth_diff_ma6'\n",
    "        ]\n",
    "        \n",
    "        available_cols = [col for col in schema_columns if col in df_save.columns]\n",
    "        df_final = df_save[available_cols].copy()\n",
    "        \n",
    "        success = db_conn.save_final_features(df_final)\n",
    "        return success\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"DB Ï†ÄÏû• Ïã§Ìå®: {e}\")\n",
    "        return False\n",
    "\n",
    "# ÌîºÏ≥ê Ï†ÄÏû• Ïã§Ìñâ\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ÏµúÏ¢Ö ÌîºÏ≥ê DB Ï†ÄÏû•\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "save_success = save_features_to_db(df, db_connection)\n",
    "if save_success:\n",
    "    print(\"DB Ï†ÄÏû• ÏôÑÎ£å\")\n",
    "else:\n",
    "    print(\"DB Ï†ÄÏû• Ïã§Ìå® - in-memory Îç∞Ïù¥ÌÑ∞Î°ú Í≥ÑÏÜç ÏßÑÌñâ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9110b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TASK 4: Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ Ìï®Ïàò Î∞è LSTM Î™®Îç∏ Ï†ïÏùò\n",
    "# ============================================================\n",
    "\n",
    "def create_sequences(X, y, seq_length):\n",
    "    \"\"\"ÏãúÍ≥ÑÏó¥ Îç∞Ïù¥ÌÑ∞Î•º LSTM ÏûÖÎ†•Ïö© ÏãúÌÄÄÏä§Î°ú Î≥ÄÌôò\"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    for i in range(len(X) - seq_length):\n",
    "        X_seq.append(X[i:i + seq_length])\n",
    "        y_seq.append(y[i + seq_length])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "def prepare_data_no_leakage(X, y, seq_length, batch_size, test_size=0.2, val_size=0.1):\n",
    "    \"\"\"Ïä§ÏºÄÏùºÎßÅ ÎàÑÏ∂úÏùÑ Î∞©ÏßÄÌïú Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ\"\"\"\n",
    "    # ÏãúÌÄÄÏä§ ÏÉùÏÑ±\n",
    "    X_seq, y_seq = create_sequences(X, y, seq_length)\n",
    "    \n",
    "    # ÏãúÍ∞Ñ ÏàúÏÑú Ïú†ÏßÄÌïòÏó¨ Î∂ÑÌï†\n",
    "    total_samples = len(X_seq)\n",
    "    test_start = int(total_samples * (1 - test_size))\n",
    "    val_start = int(test_start * (1 - val_size))\n",
    "    \n",
    "    X_train = X_seq[:val_start]\n",
    "    y_train = y_seq[:val_start]\n",
    "    X_val = X_seq[val_start:test_start]\n",
    "    y_val = y_seq[val_start:test_start]\n",
    "    X_test = X_seq[test_start:]\n",
    "    y_test = y_seq[test_start:]\n",
    "    \n",
    "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "    \n",
    "    # Ïä§ÏºÄÏùºÎßÅ (trainÏóêÏÑúÎßå fit)\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
    "    y_train_flat = y_train.reshape(-1, y_train.shape[-1])\n",
    "    \n",
    "    scaler_X.fit(X_train_flat)\n",
    "    scaler_y.fit(y_train_flat)\n",
    "    \n",
    "    # Ïä§ÏºÄÏùºÎßÅ Ï†ÅÏö©\n",
    "    X_train_scaled = np.array([scaler_X.transform(seq) for seq in X_train])\n",
    "    X_val_scaled = np.array([scaler_X.transform(seq) for seq in X_val])\n",
    "    X_test_scaled = np.array([scaler_X.transform(seq) for seq in X_test])\n",
    "    \n",
    "    y_train_scaled = scaler_y.transform(y_train)\n",
    "    y_val_scaled = scaler_y.transform(y_val)\n",
    "    y_test_scaled = scaler_y.transform(y_test)\n",
    "    \n",
    "    # Tensor Î≥ÄÌôò\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train_scaled).to(device)\n",
    "    X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "    y_val_tensor = torch.FloatTensor(y_val_scaled).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    y_test_tensor = torch.FloatTensor(y_test_scaled).to(device)\n",
    "    \n",
    "    # DataLoader ÏÉùÏÑ±\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, scaler_X, scaler_y\n",
    "\n",
    "class MultivariateLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_rate=0.2):\n",
    "        super(MultivariateLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        output = self.dropout(last_output)\n",
    "        output = self.fc(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11761c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:13:50,273] A new study created in memory with name: no-name-f1e51dbf-6f7c-4214-a08b-601b0f67c913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏµúÏ†ÅÌôî ÏãúÏûë\n",
      "============================================================\n",
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:13:55,067] Trial 0 finished with value: 2.2783424854278564 and parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.00014305097376796446, 'batch_size': 32, 'seq_length': 24, 'optimizer_type': 'Adam', 'weight_decay': 0.0007388012496161985, 'loss_function': 'MSE'}. Best is trial 0 with value: 2.2783424854278564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:13:56,515] Trial 1 finished with value: 1.026620626449585 and parameters: {'hidden_size': 32, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.00025208715781517907, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 0.00020604804798846015, 'loss_function': 'MAE'}. Best is trial 1 with value: 1.026620626449585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 120, Val: 14, Test: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:13:58,785] Trial 2 finished with value: 0.7842395305633545 and parameters: {'hidden_size': 96, 'num_layers': 2, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0042421001265853875, 'batch_size': 16, 'seq_length': 12, 'optimizer_type': 'AdamW', 'weight_decay': 0.0006430862257248823, 'loss_function': 'Huber'}. Best is trial 2 with value: 0.7842395305633545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 108, Val: 12, Test: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:01,930] Trial 3 finished with value: 0.912738561630249 and parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0015300976377717688, 'batch_size': 32, 'seq_length': 30, 'optimizer_type': 'AdamW', 'weight_decay': 4.008958080592501e-05, 'loss_function': 'Huber'}. Best is trial 2 with value: 0.7842395305633545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:02,896] Trial 4 finished with value: inf and parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout_rate': 0.4, 'learning_rate': 0.0011079238637600299, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'Adam', 'weight_decay': 0.00018856802051881794, 'loss_function': 'MSE'}. Best is trial 2 with value: 0.7842395305633545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial failed: \n",
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:03,899] Trial 5 finished with value: 0.6433122158050537 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.0031132944430118203, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 0.00016797361757880054, 'loss_function': 'Huber'}. Best is trial 5 with value: 0.6433122158050537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 116, Val: 13, Test: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:05,124] Trial 6 finished with value: 0.7060281038284302 and parameters: {'hidden_size': 32, 'num_layers': 2, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.00023955348611195168, 'batch_size': 64, 'seq_length': 18, 'optimizer_type': 'Adam', 'weight_decay': 0.00015648038850144677, 'loss_function': 'Huber'}. Best is trial 5 with value: 0.6433122158050537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 116, Val: 13, Test: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:05,819] Trial 7 finished with value: inf and parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.0010669409064523658, 'batch_size': 32, 'seq_length': 18, 'optimizer_type': 'Adam', 'weight_decay': 0.0002806687306867555, 'loss_function': 'MAE'}. Best is trial 5 with value: 0.6433122158050537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial failed: \n",
      "Train: 108, Val: 12, Test: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:06,252] Trial 8 finished with value: inf and parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.0006899455806979155, 'batch_size': 32, 'seq_length': 30, 'optimizer_type': 'Adam', 'weight_decay': 0.0002433855589350274, 'loss_function': 'Huber'}. Best is trial 5 with value: 0.6433122158050537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial failed: \n",
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:06,781] Trial 9 finished with value: inf and parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.0005833352019192215, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'Adam', 'weight_decay': 8.65644341041599e-05, 'loss_function': 'MSE'}. Best is trial 5 with value: 0.6433122158050537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial failed: \n",
      "Train: 120, Val: 14, Test: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:07,063] Trial 10 finished with value: 0.7564995884895325 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.1, 'learning_rate': 0.004937195096294677, 'batch_size': 64, 'seq_length': 12, 'optimizer_type': 'AdamW', 'weight_decay': 1.3246314519833654e-05, 'loss_function': 'Huber'}. Best is trial 5 with value: 0.6433122158050537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 116, Val: 13, Test: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:07,736] Trial 11 finished with value: 0.6822656393051147 and parameters: {'hidden_size': 32, 'num_layers': 1, 'dropout_rate': 0.1, 'learning_rate': 0.0003676987594795991, 'batch_size': 64, 'seq_length': 18, 'optimizer_type': 'AdamW', 'weight_decay': 7.683147488817311e-05, 'loss_function': 'Huber'}. Best is trial 5 with value: 0.6433122158050537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 116, Val: 13, Test: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:08,115] Trial 12 finished with value: 0.6783724427223206 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.1, 'learning_rate': 0.0023981143628413067, 'batch_size': 64, 'seq_length': 18, 'optimizer_type': 'AdamW', 'weight_decay': 5.617846885232925e-05, 'loss_function': 'Huber'}. Best is trial 5 with value: 0.6433122158050537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 116, Val: 13, Test: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:08,528] Trial 13 finished with value: 0.6694490909576416 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.1, 'learning_rate': 0.002483943936548651, 'batch_size': 64, 'seq_length': 18, 'optimizer_type': 'AdamW', 'weight_decay': 3.339916124721981e-05, 'loss_function': 'Huber'}. Best is trial 5 with value: 0.6433122158050537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:09,611] Trial 14 finished with value: 0.6222841739654541 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.002364500113408883, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 2.0985086725403104e-05, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:10,166] Trial 15 finished with value: inf and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.0025750333513609113, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 1.2367314821260336e-05, 'loss_function': 'MAE'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial failed: \n",
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:11,159] Trial 16 finished with value: 0.6257722973823547 and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.0015313636462003012, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 2.3507960386724513e-05, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:12,126] Trial 17 finished with value: 0.6481694579124451 and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.0013125576662534784, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 2.258829942554819e-05, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:12,645] Trial 18 finished with value: inf and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.0018954170054793704, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 2.7046905391181913e-05, 'loss_function': 'MSE'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial failed: \n",
      "Train: 108, Val: 12, Test: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:13,302] Trial 19 finished with value: inf and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0009231119655141637, 'batch_size': 16, 'seq_length': 30, 'optimizer_type': 'AdamW', 'weight_decay': 1.959672207916916e-05, 'loss_function': 'MAE'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial failed: \n",
      "Train: 120, Val: 14, Test: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:14,359] Trial 20 finished with value: 0.7125592231750488 and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout_rate': 0.1, 'learning_rate': 0.0005413588158247894, 'batch_size': 16, 'seq_length': 12, 'optimizer_type': 'AdamW', 'weight_decay': 5.026517909460838e-05, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:15,363] Trial 21 finished with value: 0.664354681968689 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.0033203912078536867, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 1.0304626273174093e-05, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:16,361] Trial 22 finished with value: 0.6502553820610046 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.001884096298418265, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 0.0004027237280270681, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:17,358] Trial 23 finished with value: 0.722137451171875 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.00357709889810728, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 0.00012047777287027026, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:18,347] Trial 24 finished with value: 0.6379360556602478 and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.0016796442892435008, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 1.738438972891986e-05, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:19,376] Trial 25 finished with value: 0.6289108991622925 and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0017070065959049622, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 1.809916214099414e-05, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:20,374] Trial 26 finished with value: 0.6266042590141296 and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0013623485379879788, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 1.582909201882192e-05, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:21,390] Trial 27 finished with value: 0.6581193804740906 and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0008962444444567719, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'AdamW', 'weight_decay': 3.0096474541209145e-05, 'loss_function': 'Huber'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 108, Val: 12, Test: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:21,825] Trial 28 finished with value: inf and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0007704905044372178, 'batch_size': 32, 'seq_length': 30, 'optimizer_type': 'AdamW', 'weight_decay': 5.498944318910848e-05, 'loss_function': 'MAE'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial failed: \n",
      "Train: 111, Val: 13, Test: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-22 15:14:22,883] Trial 29 finished with value: inf and parameters: {'hidden_size': 96, 'num_layers': 2, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0012898356687162436, 'batch_size': 16, 'seq_length': 24, 'optimizer_type': 'Adam', 'weight_decay': 1.3148945596714716e-05, 'loss_function': 'MSE'}. Best is trial 14 with value: 0.6222841739654541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial failed: \n",
      "\n",
      "Best Hyperparameters:\n",
      "  hidden_size: 128\n",
      "  num_layers: 1\n",
      "  dropout_rate: 0.2\n",
      "  learning_rate: 0.002364500113408883\n",
      "  batch_size: 16\n",
      "  seq_length: 24\n",
      "  optimizer_type: AdamW\n",
      "  weight_decay: 2.0985086725403104e-05\n",
      "  loss_function: Huber\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TASK 5: ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù\n",
    "# ============================================================\n",
    "\n",
    "def objective_improved(trial):\n",
    "    \"\"\"Optuna Î™©Ï†Å Ìï®Ïàò\"\"\"\n",
    "    params = {\n",
    "        'hidden_size': trial.suggest_categorical('hidden_size', [32, 64, 96, 128]),\n",
    "        'num_layers': trial.suggest_int('num_layers', 1, 2),\n",
    "        'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.4, step=0.1),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 5e-3, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64]),\n",
    "        'seq_length': trial.suggest_categorical('seq_length', [12, 18, 24, 30]),\n",
    "        'optimizer_type': trial.suggest_categorical('optimizer_type', ['Adam', 'AdamW']),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True),\n",
    "        'loss_function': trial.suggest_categorical('loss_function', ['MSE', 'MAE', 'Huber'])\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        train_loader, val_loader, _, _, _ = prepare_data_no_leakage(\n",
    "            X, y, seq_length=params['seq_length'], batch_size=params['batch_size']\n",
    "        )\n",
    "        \n",
    "        model = MultivariateLSTM(\n",
    "            input_size=X.shape[1],\n",
    "            hidden_size=params['hidden_size'],\n",
    "            num_layers=params['num_layers'],\n",
    "            output_size=y.shape[1],\n",
    "            dropout_rate=params['dropout_rate']\n",
    "        ).to(device)\n",
    "        \n",
    "        if params['optimizer_type'] == 'Adam':\n",
    "            optimizer = optim.Adam(model.parameters(), \n",
    "                                  lr=params['learning_rate'],\n",
    "                                  weight_decay=params['weight_decay'])\n",
    "        else:\n",
    "            optimizer = optim.AdamW(model.parameters(), \n",
    "                                   lr=params['learning_rate'],\n",
    "                                   weight_decay=params['weight_decay'])\n",
    "        \n",
    "        if params['loss_function'] == 'MSE':\n",
    "            criterion = nn.MSELoss()\n",
    "        elif params['loss_function'] == 'MAE':\n",
    "            criterion = nn.L1Loss()\n",
    "        else:\n",
    "            criterion = nn.HuberLoss(delta=1.0)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience = 0\n",
    "        max_patience = 10\n",
    "        \n",
    "        for epoch in range(30):\n",
    "            model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in val_loader:\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= max_patience:\n",
    "                    break\n",
    "            \n",
    "            trial.report(avg_val_loss, epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        return best_val_loss\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed: {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏµúÏ†ÅÌôî ÏãúÏûë\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=5)\n",
    ")\n",
    "\n",
    "study.optimize(objective_improved, n_trials=30, timeout=600)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "best_params = best_trial.params\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5771e00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ÏµúÏ†Å Î™®Îç∏ ÌïôÏäµ\n",
      "============================================================\n",
      "Train: 111, Val: 13, Test: 32\n",
      "Epoch   1: Train=0.332202, Val=0.632663\n",
      "Epoch   1: Train=0.332202, Val=0.632663\n",
      "Epoch  11: Train=0.188548, Val=1.304677 (10/20)\n",
      "Epoch  11: Train=0.188548, Val=1.304677 (10/20)\n",
      "Epoch  21: Train=0.122801, Val=1.223255 (20/20)\n",
      "Early stopping at epoch 21\n",
      "Best model restored with validation loss: 0.632663\n",
      "Epoch  21: Train=0.122801, Val=1.223255 (20/20)\n",
      "Early stopping at epoch 21\n",
      "Best model restored with validation loss: 0.632663\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TASK 6: ÏµúÏ†Å Î™®Îç∏ ÌïôÏäµ\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ÏµúÏ†Å Î™®Îç∏ ÌïôÏäµ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ\n",
    "train_loader, val_loader, test_loader, scaler_X, scaler_y = prepare_data_no_leakage(\n",
    "    X, y, seq_length=best_params['seq_length'], batch_size=best_params['batch_size']\n",
    ")\n",
    "\n",
    "# ÏµúÏ†Å Î™®Îç∏ ÏÉùÏÑ±\n",
    "final_model = MultivariateLSTM(\n",
    "    input_size=X.shape[1],\n",
    "    hidden_size=best_params['hidden_size'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    output_size=y.shape[1],\n",
    "    dropout_rate=best_params['dropout_rate']\n",
    ").to(device)\n",
    "\n",
    "# ÏòµÌã∞ÎßàÏù¥Ï†Ä Î∞è ÏÜêÏã§Ìï®Ïàò ÏÑ§Ï†ï\n",
    "if best_params['optimizer_type'] == 'Adam':\n",
    "    optimizer = optim.Adam(final_model.parameters(), \n",
    "                          lr=best_params['learning_rate'],\n",
    "                          weight_decay=best_params['weight_decay'])\n",
    "else:\n",
    "    optimizer = optim.AdamW(final_model.parameters(), \n",
    "                           lr=best_params['learning_rate'],\n",
    "                           weight_decay=best_params['weight_decay'])\n",
    "\n",
    "if best_params['loss_function'] == 'MSE':\n",
    "    criterion = nn.MSELoss()\n",
    "elif best_params['loss_function'] == 'MAE':\n",
    "    criterion = nn.L1Loss()\n",
    "else:\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "# Î™®Îç∏ ÌïôÏäµ\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "max_patience = 20\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(200):\n",
    "    # Training\n",
    "    final_model.train()\n",
    "    train_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = final_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(final_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    final_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            outputs = final_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = final_model.state_dict().copy()\n",
    "        print(f\"Epoch {epoch+1:3d}: Train={avg_train_loss:.6f}, Val={avg_val_loss:.6f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if epoch % 10 == 0 or patience_counter >= max_patience:\n",
    "            print(f\"Epoch {epoch+1:3d}: Train={avg_train_loss:.6f}, Val={avg_val_loss:.6f} ({patience_counter}/{max_patience})\")\n",
    "    \n",
    "    if patience_counter >= max_patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# ÏµúÏ†Å Í∞ÄÏ§ëÏπò Î≥µÏõê\n",
    "if best_model_state is not None:\n",
    "    final_model.load_state_dict(best_model_state)\n",
    "    print(f\"Best model restored with validation loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "170d9c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Î™®Îç∏ ÌèâÍ∞Ä\n",
      "============================================================\n",
      "construction_bsi_actual: RMSE=3.0729, MAE=2.6360\n",
      "base_rate: RMSE=0.1245, MAE=0.0833\n",
      "housing_sale_price: RMSE=0.3289, MAE=0.1983\n",
      "m2_growth: RMSE=0.3868, MAE=0.2829\n",
      "credit_spread: RMSE=0.0851, MAE=0.0587\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TASK 7: Î™®Îç∏ ÌèâÍ∞Ä\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Î™®Îç∏ ÌèâÍ∞Ä\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "final_model.eval()\n",
    "test_predictions = []\n",
    "test_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = final_model(batch_X)\n",
    "        test_predictions.append(outputs.cpu().numpy())\n",
    "        test_targets.append(batch_y.cpu().numpy())\n",
    "\n",
    "test_predictions = np.vstack(test_predictions)\n",
    "test_targets = np.vstack(test_targets)\n",
    "\n",
    "# Ïó≠Ï†ïÍ∑úÌôî\n",
    "test_predictions_diff = scaler_y.inverse_transform(test_predictions)\n",
    "test_targets_diff = scaler_y.inverse_transform(test_targets)\n",
    "\n",
    "# ÏÑ±Îä• Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞\n",
    "test_metrics = {}\n",
    "for i, target in enumerate(available_targets):\n",
    "    if i < test_predictions_diff.shape[1]:\n",
    "        rmse = np.sqrt(mean_squared_error(test_targets_diff[:, i], test_predictions_diff[:, i]))\n",
    "        mae = mean_absolute_error(test_targets_diff[:, i], test_predictions_diff[:, i])\n",
    "        \n",
    "        test_metrics[target] = {'RMSE': rmse, 'MAE': mae}\n",
    "        print(f\"{target}: RMSE={rmse:.4f}, MAE={mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a0f182c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ïû¨Í∑ÄÏ†Å ÎØ∏Îûò ÏòàÏ∏° (3Í∞úÏõî)\n",
      "============================================================\n",
      "\n",
      "=== Month 1 Prediction ===\n",
      "Predicted diff: [-1.7079172e+00 -1.5892196e-03  2.4743013e-01 -5.3899044e-01\n",
      "  1.6328173e-02]\n",
      "Predicted values: [67.29208279  2.49841078 93.60443013  0.46100956  5.85432817]\n",
      "Updated DataFrame shape: (181, 77)\n",
      "\n",
      "=== Month 2 Prediction ===\n",
      "Predicted diff: [-0.6047344  -0.02191541  0.29684326 -0.16727237  0.01166489]\n",
      "Predicted values: [66.68734837  2.47649537 93.90127339  0.29373719  5.86599306]\n",
      "Updated DataFrame shape: (182, 77)\n",
      "\n",
      "=== Month 3 Prediction ===\n",
      "Predicted diff: [ 0.35494673 -0.01862453  0.31391495  0.13839273  0.01259501]\n",
      "Predicted values: [67.0422951   2.45787084 94.21518835  0.43212992  5.87858807]\n",
      "Updated DataFrame shape: (183, 77)\n",
      "\n",
      "ÎØ∏Îûò 3Í∞úÏõî ÏòàÏ∏° Í≤∞Í≥º:\n",
      "            construction_bsi_actual  base_rate  housing_sale_price  m2_growth  \\\n",
      "2025-09-01                67.292083   2.498411           93.604430   0.461010   \n",
      "2025-10-01                66.687348   2.476495           93.901273   0.293737   \n",
      "2025-11-01                67.042295   2.457871           94.215188   0.432130   \n",
      "\n",
      "            credit_spread  \n",
      "2025-09-01       5.854328  \n",
      "2025-10-01       5.865993  \n",
      "2025-11-01       5.878588  \n",
      "\n",
      "ÏµúÍ∑º Ïã§Ï†úÍ∞í (ÎπÑÍµêÏö©):\n",
      "            construction_bsi_actual  base_rate  housing_sale_price  m2_growth  \\\n",
      "date                                                                            \n",
      "2025-06-01                     68.0        2.5              93.164       0.63   \n",
      "2025-07-01                     68.0        2.5              93.317       1.00   \n",
      "2025-08-01                     69.0        2.5              93.357       1.00   \n",
      "\n",
      "            credit_spread  \n",
      "date                       \n",
      "2025-06-01          5.783  \n",
      "2025-07-01          5.827  \n",
      "2025-08-01          5.838  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TASK 8: Ïû¨Í∑ÄÏ†Å ÎØ∏Îûò ÏòàÏ∏° (3Í∞úÏõî)\n",
    "# ============================================================\n",
    "\n",
    "def predict_future_recursive(model, df_original, scaler_X, scaler_y, final_features, seq_length, n_months=3):\n",
    "    \"\"\"Ïû¨Í∑ÄÏ†Å ÎØ∏Îûò ÏòàÏ∏° - Í∞Å Îã®Í≥ÑÏùò ÏòàÏ∏°Í∞íÏúºÎ°ú Îã§Ïùå ÌîºÏ≥êÎ•º ÏóÖÎç∞Ïù¥Ìä∏\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # ÌòÑÏû¨ Îç∞Ïù¥ÌÑ∞ Î≥µÏÇ¨\n",
    "    df_pred = df_original.copy()\n",
    "    future_predictions = []\n",
    "    \n",
    "    for month in range(n_months):\n",
    "        print(f\"\\n=== Month {month+1} Prediction ===\")\n",
    "        \n",
    "        # ÌòÑÏû¨ ÏãúÏ†êÏùò ÎßàÏßÄÎßâ ÏãúÌÄÄÏä§ Ï∂îÏ∂ú\n",
    "        current_features = df_pred[final_features].iloc[-seq_length:].values\n",
    "        current_features_scaled = scaler_X.transform(current_features)\n",
    "        \n",
    "        # ÏòàÏ∏° ÏàòÌñâ\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(current_features_scaled).unsqueeze(0).to(device)\n",
    "            pred_scaled = model(X_tensor).cpu().numpy()\n",
    "            pred_diff = scaler_y.inverse_transform(pred_scaled)[0]\n",
    "        \n",
    "        # Ï∞®Î∂ÑÍ∞íÏùÑ ÏõêÏãúÍ∞íÏúºÎ°ú Î≥ÄÌôò\n",
    "        last_original_values = df_pred[available_targets].iloc[-1].values\n",
    "        predicted_values = last_original_values + pred_diff\n",
    "        \n",
    "        future_predictions.append(predicted_values.copy())\n",
    "        print(f\"Predicted diff: {pred_diff}\")\n",
    "        print(f\"Predicted values: {predicted_values}\")\n",
    "        \n",
    "        # Îã§Ïùå ÏòàÏ∏°ÏùÑ ÏúÑÌï¥ DataFrame ÏóÖÎç∞Ïù¥Ìä∏\n",
    "        next_date = df_pred.index[-1] + pd.DateOffset(months=1)\n",
    "        \n",
    "        # ÏÉàÎ°úÏö¥ Ìñâ ÏÉùÏÑ± (Í∏∞Î≥∏Í∞íÏúºÎ°ú Ï¥àÍ∏∞Ìôî)\n",
    "        new_row = df_pred.iloc[-1].copy()\n",
    "        \n",
    "        # ÌÉÄÍ≤ü Î≥ÄÏàò ÏóÖÎç∞Ïù¥Ìä∏\n",
    "        for i, target in enumerate(available_targets):\n",
    "            new_row[target] = predicted_values[i]\n",
    "            new_row[f'{target}_diff'] = pred_diff[i]\n",
    "        \n",
    "        # ÏßÄÏó∞ Î≥ÄÏàò ÏóÖÎç∞Ïù¥Ìä∏ (lag features)\n",
    "        for target in available_targets:\n",
    "            if f'{target}_lag1' in new_row.index:\n",
    "                new_row[f'{target}_lag1'] = df_pred[target].iloc[-1]\n",
    "            if f'{target}_lag3' in new_row.index:\n",
    "                new_row[f'{target}_lag3'] = df_pred[target].iloc[-3] if len(df_pred) >= 3 else df_pred[target].iloc[0]\n",
    "            if f'{target}_lag6' in new_row.index:\n",
    "                new_row[f'{target}_lag6'] = df_pred[target].iloc[-6] if len(df_pred) >= 6 else df_pred[target].iloc[0]\n",
    "        \n",
    "        # Ïù¥ÎèôÌèâÍ∑† ÏóÖÎç∞Ïù¥Ìä∏ (Í∞ÑÎã®ÌôîÎêú Î≤ÑÏ†Ñ)\n",
    "        for target in available_targets:\n",
    "            diff_col = f'{target}_diff'\n",
    "            if f'{diff_col}_ma3' in new_row.index:\n",
    "                recent_diffs = df_pred[diff_col].iloc[-2:].tolist() + [pred_diff[available_targets.index(target)]]\n",
    "                new_row[f'{diff_col}_ma3'] = np.mean(recent_diffs)\n",
    "            if f'{diff_col}_ma6' in new_row.index:\n",
    "                recent_diffs = df_pred[diff_col].iloc[-5:].tolist() + [pred_diff[available_targets.index(target)]]\n",
    "                new_row[f'{diff_col}_ma6'] = np.mean(recent_diffs)\n",
    "        \n",
    "        # DataFrameÏóê ÏÉàÎ°úÏö¥ Ìñâ Ï∂îÍ∞Ä\n",
    "        new_row.name = next_date\n",
    "        df_pred = pd.concat([df_pred, new_row.to_frame().T])\n",
    "        \n",
    "        print(f\"Updated DataFrame shape: {df_pred.shape}\")\n",
    "    \n",
    "    return np.array(future_predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Ïû¨Í∑ÄÏ†Å ÎØ∏Îûò ÏòàÏ∏° (3Í∞úÏõî)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ïû¨Í∑ÄÏ†Å ÏòàÏ∏° ÏàòÌñâ\n",
    "future_predictions_recursive = predict_future_recursive(\n",
    "    final_model, df, scaler_X, scaler_y, final_features, \n",
    "    best_params['seq_length'], n_months=3\n",
    ")\n",
    "\n",
    "# ÏòàÏ∏° Í≤∞Í≥ºÎ•º DataFrameÏúºÎ°ú Î≥ÄÌôò\n",
    "last_date = df.index[-1]\n",
    "future_dates = pd.date_range(start=last_date + pd.DateOffset(months=1), periods=3, freq='MS')\n",
    "\n",
    "predictions_df = pd.DataFrame(\n",
    "    future_predictions_recursive,\n",
    "    columns=available_targets,\n",
    "    index=future_dates\n",
    ")\n",
    "\n",
    "print(\"\\nÎØ∏Îûò 3Í∞úÏõî ÏòàÏ∏° Í≤∞Í≥º:\")\n",
    "print(predictions_df)\n",
    "\n",
    "# ÏµúÍ∑º Ïã§Ï†úÍ∞íÍ≥º ÎπÑÍµê\n",
    "print(\"\\nÏµúÍ∑º Ïã§Ï†úÍ∞í (ÎπÑÍµêÏö©):\")\n",
    "recent_values = df[available_targets].tail(3)\n",
    "print(recent_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b625728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ÏòàÏ∏° Í≤∞Í≥º DB Ï†ÄÏû•\n",
      "============================================================\n",
      "Î™®Îç∏ ÏòàÏ∏° Í≤∞Í≥º 3Í±¥ Ï†ÄÏû• ÏôÑÎ£å\n",
      "ÏòàÏ∏° Í≤∞Í≥º DB Ï†ÄÏû• ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TASK 9: ÏòàÏ∏° Í≤∞Í≥º DB Ï†ÄÏû•\n",
    "# ============================================================\n",
    "\n",
    "def save_predictions_to_db(predictions_df, db_conn):\n",
    "    \"\"\"ÏòàÏ∏° Í≤∞Í≥ºÎ•º model_output ÌÖåÏù¥Î∏îÏóê Ï†ÄÏû•\"\"\"\n",
    "    if not db_conn:\n",
    "        print(\"DB Ïó∞Í≤∞ ÏóÜÏùå - ÌååÏùºÎ°úÎßå Ï†ÄÏû•\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ (DDL Ïä§ÌÇ§ÎßàÏóê ÎßûÏ∂§)\n",
    "        pred_save = predictions_df.copy()\n",
    "        pred_save.reset_index(inplace=True)\n",
    "        \n",
    "        # Ïù∏Îç±Ïä§ Ïª¨ÎüºÎ™ÖÏùÑ 'date'Î°ú Î≥ÄÍ≤Ω\n",
    "        if 'index' in pred_save.columns:\n",
    "            pred_save.rename(columns={'index': 'date'}, inplace=True)\n",
    "        \n",
    "        pred_save['date'] = pd.to_datetime(pred_save['date']).dt.strftime('%Y%m')\n",
    "        \n",
    "        # credit_spread Ïª¨Îüº Ï∂îÍ∞Ä (ÏóÜÎäî Í≤ΩÏö∞)\n",
    "        if 'credit_spread' not in pred_save.columns:\n",
    "            pred_save['credit_spread'] = 0.0\n",
    "        \n",
    "        success = db_conn.save_model_output(pred_save)\n",
    "        return success\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ÏòàÏ∏° Í≤∞Í≥º Ï†ÄÏû• Ïã§Ìå®: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ÏòàÏ∏° Í≤∞Í≥º DB Ï†ÄÏû•\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pred_save_success = save_predictions_to_db(predictions_df, db_connection)\n",
    "if pred_save_success:\n",
    "    print(\"ÏòàÏ∏° Í≤∞Í≥º DB Ï†ÄÏû• ÏôÑÎ£å\")\n",
    "else:\n",
    "    print(\"ÏòàÏ∏° Í≤∞Í≥º Ï†ÄÏû• Ïã§Ìå®\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d499a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Î™®Îç∏ ÏïÑÌã∞Ìå©Ìä∏ Ï†ÄÏû•\n",
      "============================================================\n",
      "Î™®Îç∏ ÏïÑÌã∞Ìå©Ìä∏ Ï†ÄÏû•: lstm_model_artifacts.pkl\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TASK 10: Î™®Îç∏ ÏïÑÌã∞Ìå©Ìä∏ Ï†ÄÏû•\n",
    "# ============================================================\n",
    "\n",
    "def save_model_artifacts(artifacts, filepath='output/lstm_model_artifacts.pkl'):\n",
    "    \"\"\"Î™®Îç∏ ÏïÑÌã∞Ìå©Ìä∏Î•º ÌååÏùºÎ°ú Ï†ÄÏû•\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(artifacts, f)\n",
    "        print(f\"Î™®Îç∏ ÏïÑÌã∞Ìå©Ìä∏ Ï†ÄÏû•: {filepath}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"ÏïÑÌã∞Ìå©Ìä∏ Ï†ÄÏû• Ïã§Ìå®: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Î™®Îç∏ ÏïÑÌã∞Ìå©Ìä∏ Ï†ÄÏû•\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Î≤†Ïù¥Ïä§ÎùºÏù∏ Î©îÌä∏Î¶≠ (Í∞ÑÎã®Ìïú Î≤ÑÏ†Ñ)\n",
    "baseline_metrics = {}\n",
    "for target in available_targets:\n",
    "    naive_pred = np.full(len(test_targets_diff), df[target].iloc[-1])\n",
    "    if len(test_targets_diff) > 0:\n",
    "        target_idx = available_targets.index(target)\n",
    "        rmse = np.sqrt(mean_squared_error(test_targets_diff[:, target_idx], naive_pred))\n",
    "        baseline_metrics[f'{target}_naive_rmse'] = rmse\n",
    "\n",
    "# ÏïÑÌã∞Ìå©Ìä∏ Íµ¨ÏÑ±\n",
    "artifacts = {\n",
    "    \"model_state_dict\": {k: v.cpu() for k, v in final_model.state_dict().items()},\n",
    "    \"hyperparameters\": best_params,\n",
    "    \"scaler_X\": scaler_X,\n",
    "    \"scaler_y\": scaler_y,\n",
    "    \"final_features\": final_features,\n",
    "    \"target_columns\": available_targets,\n",
    "    \"test_metrics\": test_metrics,\n",
    "    \"baseline_metrics\": baseline_metrics,\n",
    "    \"training_losses\": train_losses,\n",
    "    \"validation_losses\": val_losses\n",
    "}\n",
    "\n",
    "artifact_saved = save_model_artifacts(artifacts, os.path.join('output', 'lstm_model_artifacts.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d3285",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3439066478.py, line 86)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mplt.title(f'{target}: Historical vs Future Predictions')plt.show()\u001b[39m\n                                                            ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TASK 11: Í≤∞Í≥º ÌååÏùº Ï†ÄÏû• Î∞è ÏãúÍ∞ÅÌôî\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Í≤∞Í≥º ÌååÏùº Ï†ÄÏû•\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# output Ìè¥Îçî ÏÉùÏÑ±\n",
    "import os\n",
    "output_dir = 'output'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"‚úì {output_dir} Ìè¥Îçî ÏÉùÏÑ±\")\n",
    "\n",
    "try:\n",
    "    # ÏòàÏ∏° Í≤∞Í≥º Ï†ÄÏû•\n",
    "    predictions_df.to_csv(os.path.join(output_dir, 'lstm_predictions_3months.csv'))\n",
    "    print(f\"ÏòàÏ∏° Í≤∞Í≥º Ï†ÄÏû•: {output_dir}/lstm_predictions_3months.csv\")\n",
    "    \n",
    "    # ÏÑ±Îä• Î©îÌä∏Î¶≠ Ï†ÄÏû•\n",
    "    metrics_df = pd.DataFrame(test_metrics).T\n",
    "    metrics_df.to_csv(os.path.join(output_dir, 'lstm_performance_metrics.csv'))\n",
    "    print(f\"ÏÑ±Îä• Î©îÌä∏Î¶≠ Ï†ÄÏû•: {output_dir}/lstm_performance_metrics.csv\")\n",
    "    \n",
    "    # Î™®Îç∏ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû•\n",
    "    torch.save({\n",
    "        'model_state_dict': final_model.state_dict(),\n",
    "        'hyperparameters': best_params,\n",
    "        'scaler_X': scaler_X,\n",
    "        'scaler_y': scaler_y\n",
    "    }, os.path.join(output_dir, 'lstm_model_checkpoint.pth'))\n",
    "    print(f\"Î™®Îç∏ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû•: {output_dir}/lstm_model_checkpoint.pth\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ÌååÏùº Ï†ÄÏû• Ïò§Î•ò: {e}\")\n",
    "\n",
    "# Í∞ÑÎã®Ìïú ÏãúÍ∞ÅÌôî\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ÏãúÍ∞ÅÌôî\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ÌïôÏäµ Í≥°ÏÑ†\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training History')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# ÏòàÏ∏° vs Ïã§Ï†ú (Ï≤´ Î≤àÏß∏ ÌÉÄÍ≤üÎßå)\n",
    "plt.subplot(1, 2, 2)\n",
    "if len(test_predictions_diff) > 0 and len(available_targets) > 0:\n",
    "    target_idx = 0\n",
    "    plt.scatter(test_targets_diff[:, target_idx], test_predictions_diff[:, target_idx], alpha=0.6)\n",
    "    min_val = min(test_targets_diff[:, target_idx].min(), test_predictions_diff[:, target_idx].min())\n",
    "    max_val = max(test_targets_diff[:, target_idx].max(), test_predictions_diff[:, target_idx].max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title(f'{available_targets[0]} Prediction')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ÎØ∏Îûò ÏòàÏ∏° ÏãúÍ∞ÅÌôî\n",
    "plt.figure(figsize=(15, 3*len(available_targets)))\n",
    "for i, target in enumerate(available_targets):\n",
    "    plt.subplot(len(available_targets), 1, i+1)\n",
    "    \n",
    "    # ÏµúÍ∑º 24Í∞úÏõî Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞\n",
    "    recent_data = df[target].tail(24)\n",
    "    future_data = predictions_df[target]\n",
    "    \n",
    "    plt.plot(recent_data.index, recent_data.values, 'o-', label='Historical', alpha=0.7)\n",
    "    plt.plot(future_data.index, future_data.values, 's-', label='Predicted', color='red', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(target)\n",
    "    plt.title(f'{target}: Historical vs Future Predictions')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cb41cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LSTM Î™®Îç∏ÎßÅ ÌååÏù¥ÌîÑÎùºÏù∏ ÏôÑÎ£å\n",
      "============================================================\n",
      "ÏÉùÏÑ±Îêú ÌååÏùº:\n",
      "  ‚Ä¢ output/lstm_predictions_3months.csv\n",
      "  ‚Ä¢ output/lstm_performance_metrics.csv\n",
      "  ‚Ä¢ output/lstm_model_checkpoint.pth\n",
      "  ‚Ä¢ output/lstm_model_artifacts.pkl\n",
      "\n",
      "ÏµúÏ¢Ö ÏÑ±Îä•:\n",
      "  construction_bsi_actual: RMSE = 3.0729, MAE = 2.6360\n",
      "  base_rate: RMSE = 0.1245, MAE = 0.0833\n",
      "  housing_sale_price: RMSE = 0.3289, MAE = 0.1983\n",
      "  m2_growth: RMSE = 0.3868, MAE = 0.2829\n",
      "  credit_spread: RMSE = 0.0851, MAE = 0.0587\n",
      "\n",
      "ÏòàÏ∏° Í≤∞Í≥º ÏöîÏïΩ:\n",
      "  construction_bsi_actual: ÌòÑÏû¨ 69.000 ‚Üí 3Í∞úÏõî ÌõÑ ÌèâÍ∑† 67.007 (Î≥ÄÌôî: -1.993)\n",
      "  base_rate: ÌòÑÏû¨ 2.500 ‚Üí 3Í∞úÏõî ÌõÑ ÌèâÍ∑† 2.478 (Î≥ÄÌôî: -0.022)\n",
      "  housing_sale_price: ÌòÑÏû¨ 93.357 ‚Üí 3Í∞úÏõî ÌõÑ ÌèâÍ∑† 93.907 (Î≥ÄÌôî: +0.550)\n",
      "  m2_growth: ÌòÑÏû¨ 1.000 ‚Üí 3Í∞úÏõî ÌõÑ ÌèâÍ∑† 0.396 (Î≥ÄÌôî: -0.604)\n",
      "  credit_spread: ÌòÑÏû¨ 5.838 ‚Üí 3Í∞úÏõî ÌõÑ ÌèâÍ∑† 5.866 (Î≥ÄÌôî: +0.028)\n",
      "MySQL Ïó∞Í≤∞ Ìï¥Ï†ú\n",
      "\n",
      "DB Ïó∞Í≤∞ Ï¢ÖÎ£å ÏôÑÎ£å\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TASK 12: ÏµúÏ¢Ö Ï†ïÎ¶¨ Î∞è DB Ïó∞Í≤∞ Ï¢ÖÎ£å\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LSTM Î™®Îç∏ÎßÅ ÌååÏù¥ÌîÑÎùºÏù∏ ÏôÑÎ£å\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"ÏÉùÏÑ±Îêú ÌååÏùº:\")\n",
    "print(\"  ‚Ä¢ output/lstm_predictions_3months.csv\")\n",
    "print(\"  ‚Ä¢ output/lstm_performance_metrics.csv\") \n",
    "print(\"  ‚Ä¢ output/lstm_model_checkpoint.pth\")\n",
    "print(\"  ‚Ä¢ output/lstm_model_artifacts.pkl\")\n",
    "\n",
    "print(f\"\\nÏµúÏ¢Ö ÏÑ±Îä•:\")\n",
    "for target, metrics in test_metrics.items():\n",
    "    print(f\"  {target}: RMSE = {metrics['RMSE']:.4f}, MAE = {metrics['MAE']:.4f}\")\n",
    "\n",
    "print(f\"\\nÏòàÏ∏° Í≤∞Í≥º ÏöîÏïΩ:\")\n",
    "for i, target in enumerate(available_targets):\n",
    "    last_actual = df[target].iloc[-1]\n",
    "    pred_values = predictions_df[target].values\n",
    "    avg_change = np.mean(pred_values - last_actual)\n",
    "    print(f\"  {target}: ÌòÑÏû¨ {last_actual:.3f} ‚Üí 3Í∞úÏõî ÌõÑ ÌèâÍ∑† {np.mean(pred_values):.3f} (Î≥ÄÌôî: {avg_change:+.3f})\")\n",
    "\n",
    "# DB Ïó∞Í≤∞ Ï†ïÎ¶¨\n",
    "if db_connection:\n",
    "    db_connection.disconnect()\n",
    "    print(\"\\nDB Ïó∞Í≤∞ Ï¢ÖÎ£å ÏôÑÎ£å\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
