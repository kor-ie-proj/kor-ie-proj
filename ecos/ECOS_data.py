import os, csv, requests, time
from datetime import datetime
from dotenv import load_dotenv
import sys
import pandas as pd

# DB ì—°ê²°ì„ ìœ„í•œ ê²½ë¡œ ì¶”ê°€ ë° import
try:
    from db_query import DatabaseConnection
except ImportError:
    # ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ DB í´ë”ì—ì„œ db_query ëª¨ë“ˆ ì°¾ê¸°
    current_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(current_dir)
    db_path = os.path.join(parent_dir, 'DB')
    if db_path not in sys.path:
        sys.path.insert(0, db_path)
    try:
        from db_query import DatabaseConnection
    except ImportError:
        print("ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        DatabaseConnection = None

BASE = "https://ecos.bok.or.kr/api"

def now_ym(): 
    return datetime.now().strftime("%Y%m")

def ecos(key, stat, period, start, end, item=None):
    # í° ê¸°ê°„ì„ ì—¬ëŸ¬ ë²ˆì— ë‚˜ëˆ„ì–´ í˜¸ì¶œ (1ë…„ì”©)
    all_rows = []
    
    start_year = int(start[:4])
    start_month = int(start[4:])
    end_year = int(end[:4])
    end_month = int(end[4:])
    
    # 1ë…„ì”© ë‚˜ëˆ„ì–´ì„œ í˜¸ì¶œ
    for year in range(start_year, end_year + 1):
        # ì‹œì‘ì›”ê³¼ ëì›” ê³„ì‚°
        if year == start_year:
            chunk_start = start
        else:
            chunk_start = f"{year}01"
            
        if year == end_year:
            chunk_end = end
        else:
            chunk_end = f"{year}12"
        
        url = f"{BASE}/StatisticSearch/{key}/json/kr/1/10000/{stat}/{period}/{chunk_start}/{chunk_end}/"
        if item: url += f"{item}/"
        
        print(f"API í˜¸ì¶œ: {chunk_start} ~ {chunk_end}")
        
        # ì¬ì‹œë„ ë¡œì§
        max_retries = 3
        for attempt in range(max_retries):
            try:
                time.sleep(2)  # 2ì´ˆ ëŒ€ê¸°
                r = requests.get(url, timeout=30)
                r.raise_for_status()  # HTTP ì˜¤ë¥˜ ì²´í¬
                
                j = r.json()
                if "StatisticSearch" in j and "row" in j["StatisticSearch"]:
                    rows = j["StatisticSearch"]["row"]
                    all_rows.extend(rows)
                    print(f"  ì„±ê³µ: {len(rows)}ê°œ í–‰ ìˆ˜ì§‘")
                    break
                elif "RESULT" in j and j["RESULT"]["CODE"] != "INFO-000":
                    print(f"  API ì˜¤ë¥˜: {j['RESULT']['MESSAGE']}")
                    break
                else:
                    print(f"  ë°ì´í„° ì—†ìŒ")
                    break
                    
            except requests.exceptions.RequestException as e:
                print(f"  ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)  # ì‹¤íŒ¨ì‹œ 5ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
                else:
                    print(f"  ìµœì¢… ì‹¤íŒ¨: {chunk_start} ~ {chunk_end}")
            except Exception as e:
                print(f"  ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                break
    
    print(f"ì´ ìˆ˜ì§‘ëœ í–‰ ìˆ˜: {len(all_rows)}")
    return all_rows

def pick(rows, keys):
    if not keys: return rows
    return [r for r in rows if r.get("ITEM_NAME1") in keys]

def collect_multiple_items(key, stat, period, start, end, item_names):
    """ì—¬ëŸ¬ í’ˆëª©ì„ ìˆ˜ì§‘í•˜ì—¬ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    all_rows = []
    rows = ecos(key, stat, period, start, end, None)
    
    for item_name in item_names:
        matching_rows = [r for r in rows if r.get("ITEM_NAME1") == item_name]
        all_rows.extend(matching_rows)
        print(f"  {item_name}: {len(matching_rows)}ê°œ í–‰")
    
    return all_rows

def calculate_growth_rate(rows):
    """ì „ê¸°ëŒ€ë¹„ ì¦ê°€ìœ¨ ê³„ì‚°"""
    rows = sorted(rows, key=lambda x: x["TIME"])
    growth_rows = []
    
    for i in range(1, len(rows)):
        try:
            prev_val = float(rows[i-1]["DATA_VALUE"])
            curr_val = float(rows[i]["DATA_VALUE"])
            if prev_val != 0:
                growth_rate = ((curr_val - prev_val) / prev_val) * 100
                growth_row = rows[i].copy()
                growth_row["DATA_VALUE"] = f"{growth_rate:.2f}"
                growth_row["UNIT_NAME"] = "%"
                growth_rows.append(growth_row)
        except:
            continue
    return growth_rows

def create_merged_csv(all_data):
    """ëª¨ë“  ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ í†µí•© CSV íŒŒì¼ë¡œ ì €ì¥ (DBì™€ ë³„ê°œ)"""
    print("\n=== Merged CSV íŒŒì¼ ìƒì„± ì‹œì‘ ===")
    
    try:
        # ëª¨ë“  ë°ì´í„°ë¥¼ ë‚ ì§œë³„ë¡œ í†µí•©
        merged_data = {}
        
        for file_info in all_data:
            file_name = file_info['file_name'].replace('.csv', '')
            rows = file_info['data']
            
            for row in rows:
                date = row.get('TIME')
                item_name = row.get('ITEM_NAME1', '')
                value_str = row.get('DATA_VALUE', '')
                unit = row.get('UNIT_NAME', '')
                stat_name = row.get('STAT_NAME', '')
                
                if date not in merged_data:
                    merged_data[date] = {}
                
                # ì»¬ëŸ¼ëª… ìƒì„± (íŒŒì¼ëª…_í•­ëª©ëª… í˜•íƒœ)
                if file_name.startswith('market_rate_'):
                    column_name = f"{item_name}"
                elif file_name.startswith('exchange_'):
                    column_name = f"{item_name}"
                elif file_name.startswith('import_price_'):
                    column_name = f"{item_name}_import_price"
                elif file_name.startswith('ppi_'):
                    column_name = f"{item_name}_ppi"
                elif item_name:
                    column_name = f"{item_name}"
                else:
                    column_name = file_name
                
                try:
                    value = float(value_str) if value_str else None
                except:
                    value = None
                
                merged_data[date][column_name] = value
        
        # DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame.from_dict(merged_data, orient='index')
        df.index.name = 'date'
        df = df.sort_index()
        
        # merged CSV íŒŒì¼ ì €ì¥ ê²½ë¡œ
        current_dir = os.path.dirname(os.path.abspath(__file__))
        merged_file_path = os.path.join(current_dir, "economic_data_merged.csv")
        
        # CSV íŒŒì¼ë¡œ ì €ì¥
        df.to_csv(merged_file_path, encoding='utf-8-sig')
        
        print(f"   Merged CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {merged_file_path}")
        print(f"   - ë°ì´í„° ê¸°ê°„: {df.index.min()} ~ {df.index.max()}")
        print(f"   - ì´ í–‰ ìˆ˜: {len(df)}")
        print(f"   - ì´ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
        
        # ì»¬ëŸ¼ ëª©ë¡ ì¶œë ¥ (ì²˜ìŒ 10ê°œë§Œ)
        print(f"   - ì£¼ìš” ì»¬ëŸ¼: {list(df.columns)[:10]}...")
        
        return True
        
    except Exception as e:
        print(f" Merged CSV íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False

def save_to_database(all_data):
    """ìˆ˜ì§‘ëœ ëª¨ë“  ë°ì´í„°ë¥¼ MySQL ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ìŠ¤í‚¤ë§ˆë³„ ì»¬ëŸ¼ ë§¤í•‘)"""
    if DatabaseConnection is None:
        print("ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CSV íŒŒì¼ë¡œë§Œ ì €ì¥í•©ë‹ˆë‹¤.")
        return False
        
    try:
        db = DatabaseConnection()
        if not db.connect():
            print("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨ - CSV íŒŒì¼ë¡œë§Œ ì €ì¥í•©ë‹ˆë‹¤.")
            return False
        
        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ì™„ì „ êµì²´)
        delete_query = "DELETE FROM ecos_data"
        cursor = db.connection.cursor()
        cursor.execute(delete_query)
        print("ê¸°ì¡´ ECOS ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
        
        # í•­ëª©ëª… â†’ ì»¬ëŸ¼ëª… ë§¤í•‘ (DDLì˜ ì»¬ëŸ¼ëª… ì¼ì¹˜)
        item_to_column = {
            'ê¸°ì¤€ê¸ˆë¦¬': 'base_rate',
            'í˜„ì¬ìƒí™œí˜•í¸CSI': 'ccsi', 
            'ì—…í™©ì‹¤ì BSI 1)': 'construction_bsi_actual',
            'ì—…í™©ì „ë§BSI 1)': 'construction_bsi_forecast',
            'ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜': 'cpi',
            'ê²½ì œì‹¬ë¦¬ì§€ìˆ˜(ì›ê³„ì—´)': 'esi',
            'ì›/ë‹¬ëŸ¬(ì¢…ê°€ 15:30)': 'exchange_usd_krw_close',
            'ì´ì§€ìˆ˜_housing_lease_price': 'housing_lease_price',
            'ì´ì§€ìˆ˜_housing_sale_price': 'housing_sale_price', 
            'ë¹„ê¸ˆì†ê´‘ë¬¼_import': 'import_price_non_metal_mineral',
            'ì² ê°•1ì°¨ì œí’ˆ_import': 'import_price_steel_primary',
            'ì„ í–‰ì§€ìˆ˜ìˆœí™˜ë³€ë™ì¹˜': 'leading_index',
            'M2(í‰ì”, ê³„ì ˆì¡°ì •ê³„ì—´)': 'm2_growth',
            'êµ­ê³ ì±„(10ë…„)': 'market_rate_treasury_bond_10yr',
            'êµ­ê³ ì±„(3ë…„)': 'market_rate_treasury_bond_3yr',
            'íšŒì‚¬ì±„(3ë…„, AA-)': 'market_rate_corporate_bond_3yr_AA',
            'íšŒì‚¬ì±„(3ë…„, BBB-)': 'market_rate_corporate_bond_3yr_BBB',
            'ë¹„ê¸ˆì†ê´‘ë¬¼_ppi': 'ppi_non_metal_mineral',
            'ì² ê°•1ì°¨ì œí’ˆ_ppi': 'ppi_steel_primary'
        }
        
        # íŒŒì¼ëª… â†’ í•­ëª©ëª… ë§¤í•‘ (ë‹¨ì¼ íŒŒì¼ë“¤)
        file_to_item = {
            'base_rate': 'ê¸°ì¤€ê¸ˆë¦¬',
            'cpi': 'ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜', 
            'esi': 'ê²½ì œì‹¬ë¦¬ì§€ìˆ˜(ì›ê³„ì—´)',
            'ccsi': 'í˜„ì¬ìƒí™œí˜•í¸CSI',
            'construction_bsi_actual': 'ì—…í™©ì‹¤ì BSI 1)',
            'construction_bsi_forecast': 'ì—…í™©ì „ë§BSI 1)',
            'housing_sale_price': 'ì´ì§€ìˆ˜',
            'housing_lease_price': 'ì´ì§€ìˆ˜',
            'leading_index': 'ì„ í–‰ì§€ìˆ˜ìˆœí™˜ë³€ë™ì¹˜',
            'm2_growth': 'M2(í‰ì”, ê³„ì ˆì¡°ì •ê³„ì—´)'
        }
        
        print("=== ë°ì´í„° ë§¤í•‘ ì‹œì‘ ===")
        
        # ë‚ ì§œë³„ë¡œ ë°ì´í„° ê·¸ë£¹í™”
        date_data = {}
        for file_info in all_data:
            file_name = file_info['file_name'].replace('.csv', '')
            rows = file_info['data']
            
            print(f"\nğŸ”„ {file_name} ì²˜ë¦¬ ì¤‘... ({len(rows)}ê°œ í–‰)")
            
            for row in rows:
                date = row.get('TIME')
                item_name = row.get('ITEM_NAME1', '')
                value_str = row.get('DATA_VALUE', '')
                
                # ë§¤í•‘í•  í•­ëª©ëª… ê²°ì •
                if file_name in file_to_item:
                    # ë‹¨ì¼ íŒŒì¼ì˜ ê²½ìš°
                    if file_name.startswith('housing_'):
                        # ì£¼íƒê°€ê²©ì§€ìˆ˜ëŠ” íŠ¹ë³„ ì²˜ë¦¬
                        item_key = f"ì´ì§€ìˆ˜_{file_name}"
                    else:
                        item_key = file_to_item[file_name]
                elif file_name.startswith('market_rate_'):
                    # ê¸ˆë¦¬ íŒŒì¼
                    item_key = item_name
                elif file_name.startswith('exchange_'):
                    # í™˜ìœ¨ íŒŒì¼
                    item_key = item_name
                elif file_name.startswith('import_price_'):
                    # ìˆ˜ì…ë¬¼ê°€ì§€ìˆ˜
                    if item_name in ['ë¹„ê¸ˆì†ê´‘ë¬¼', 'ì² ê°•1ì°¨ì œí’ˆ']:
                        item_key = f"{item_name}_import"
                    else:
                        item_key = item_name
                elif file_name.startswith('ppi_'):
                    # ìƒì‚°ìë¬¼ê°€ì§€ìˆ˜
                    if item_name in ['ë¹„ê¸ˆì†ê´‘ë¬¼', 'ì² ê°•1ì°¨ì œí’ˆ']:
                        item_key = f"{item_name}_ppi"
                    else:
                        item_key = item_name
                else:
                    item_key = item_name
                
                if date not in date_data:
                    date_data[date] = {}
                
                try:
                    value = float(value_str) if value_str else None
                except:
                    value = None
                    
                date_data[date][item_key] = value
                
                # ë§¤í•‘ í™•ì¸ìš© ë¡œê·¸ (ì²˜ìŒ ëª‡ ê°œë§Œ)
                if len(date_data) <= 3:
                    if item_key in item_to_column:
                        column_name = item_to_column[item_key]
                        print(f"{item_key} â†’ {column_name}: {value}")
                    else:
                        print(f"ë§¤í•‘ë˜ì§€ ì•Šì€ í•­ëª©: {item_key}")
        
        print(f"\nì´ {len(date_data)}ê°œ ë‚ ì§œì˜ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
        
        # ì‹¤ì œ ì €ì¥ë  ë°ì´í„° ìƒ˜í”Œ í™•ì¸
        print("\n=== ì €ì¥ë  ë°ì´í„° ìƒ˜í”Œ ===")
        sample_dates = list(date_data.keys())[:3]
        for date in sample_dates:
            print(f"ë‚ ì§œ {date}:")
            for item_key, value in date_data[date].items():
                if item_key in item_to_column:
                    print(f"  {item_key} = {value}")
        
        # MySQL ìŠ¤í‚¤ë§ˆì— ë§ì¶° ë°°ì¹˜ ì‚½ì… (ì„œë²„ ë¶€í•˜ ë°©ì§€)
        insert_query = """
        INSERT INTO ecos_data (
            date, base_rate, ccsi, construction_bsi_actual, construction_bsi_forecast,
            cpi, esi, exchange_usd_krw_close, housing_lease_price, housing_sale_price,
            import_price_non_metal_mineral, import_price_steel_primary, leading_index, m2_growth,
            market_rate_treasury_bond_10yr, market_rate_treasury_bond_3yr, market_rate_corporate_bond_3yr_AA, market_rate_corporate_bond_3yr_BBB,
            ppi_non_metal_mineral, ppi_steel_primary
        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        
        data_tuples = []
        for date, values in date_data.items():
            row_data = [date]
            for item_key, column_name in item_to_column.items():
                row_data.append(values.get(item_key, None))
            data_tuples.append(tuple(row_data))
        
        print(f"\n=== ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹œì‘ ({len(data_tuples)}ê°œ í–‰) ===")
        
        # ë°°ì¹˜ í¬ê¸° ì„¤ì • (ì„œë²„ ë¶€í•˜ ë°©ì§€)
        batch_size = 10  # í•œ ë²ˆì— 10ê°œì”© ì €ì¥
        total_batches = (len(data_tuples) + batch_size - 1) // batch_size
        
        import time
        
        for i in range(0, len(data_tuples), batch_size):
            batch = data_tuples[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            
            print(f" ë°°ì¹˜ {batch_num}/{total_batches} ì €ì¥ ì¤‘... ({len(batch)}ê°œ í–‰)")
            
            cursor.executemany(insert_query, batch)
            db.connection.commit()
            
            print(f" ë°°ì¹˜ {batch_num} ì €ì¥ ì™„ë£Œ")
            
            # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸° ì‹œê°„ (ë§ˆì§€ë§‰ ë°°ì¹˜ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ)
            if i + batch_size < len(data_tuples):
                time.sleep(3)
        
        print(f"ë°ì´í„°ë² ì´ìŠ¤ì— ì´ {len(data_tuples)}ê°œ í–‰ ë°°ì¹˜ ì €ì¥ ì™„ë£Œ")
        
        cursor.close()
        db.disconnect()
        return True
        
    except Exception as e:
        print(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        if 'cursor' in locals():
            cursor.close()
        if 'db' in locals():
            db.disconnect()
        return False
        
        data_tuples = []
        for date, values in date_data.items():
            row_data = [date]
            for item_key, column_name in item_to_column.items():
                row_data.append(values.get(item_key, None))
            data_tuples.append(tuple(row_data))
        
        cursor.executemany(insert_query, data_tuples)
        db.connection.commit()
        
        print(f"ë°ì´í„°ë² ì´ìŠ¤ì— {len(data_tuples)}ê°œ í–‰ ì €ì¥ ì™„ë£Œ")
        
        cursor.close()
        db.disconnect()
        return True
        
    except Exception as e:
        print(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if 'cursor' in locals():
            cursor.close()
        if 'db' in locals():
            db.disconnect()
        return False

def save(path, rows):
    if not rows: 
        print(f"  ê²½ê³ : {path} - ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return []
    
    # ì¤‘ë³µ ì œê±°
    seen = set()
    unique_rows = []
    for r in rows:
        key = f"{r['TIME']}_{r.get('ITEM_NAME1', '')}"
        if key not in seen:
            seen.add(key)
            unique_rows.append(r)
    
    rows = sorted(unique_rows, key=lambda x: x["TIME"])
    
    # CSV íŒŒì¼ ì €ì¥ (ë°±ì—…ìš©)
    with open(path, "w", newline="", encoding="utf-8-sig") as f:
        w = csv.writer(f)
        w.writerow(["date","value","unit","stat_name","item_name"])
        for r in rows:
            w.writerow([r.get("TIME"), r.get("DATA_VALUE"), r.get("UNIT_NAME"), r.get("STAT_NAME"), r.get("ITEM_NAME1")])
    
    print(f"  CSV ì €ì¥ ì™„ë£Œ: {path} ({len(rows)}ê°œ í–‰)")
    return rows

def save_individual(base_name, rows):
    """ê°œë³„ íŒŒì¼ë¡œ ì €ì¥"""
    if not rows: return []
    
    items = {}
    for r in rows:
        item = r.get("ITEM_NAME1", "")
        if item:
            if item not in items: items[item] = []
            items[item].append(r)
    
    all_saved_rows = []
    for item, item_rows in items.items():
        safe_name = item.replace("(", "").replace(")", "").replace(",", "").replace("/", "_").replace(":", "_").replace(" ", "_").replace("-", "_")
        # í˜„ì¬ ecos í´ë” ì•ˆì˜ economic_data í´ë” ì‚¬ìš©
        current_dir = os.path.dirname(os.path.abspath(__file__))
        filename = os.path.join(current_dir, "economic_data", f"{base_name}_{safe_name}.csv")
        saved_rows = save(filename, item_rows)
        all_saved_rows.extend(saved_rows)
    
    return all_saved_rows

if __name__ == "__main__":
    load_dotenv()
    key = os.getenv("ECOS_API_KEY")
    start, end = "201510", now_ym()
    
    # ecos í´ë” ì•ˆì— economic_data í´ë” ìƒì„±
    current_dir = os.path.dirname(os.path.abspath(__file__))
    economic_data_dir = os.path.join(current_dir, "economic_data")
    os.makedirs(economic_data_dir, exist_ok=True)
    
    # ëª¨ë“  ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    all_collected_data = []
    
    # ë‹¨ì¼ íŒŒì¼
    single_files = [
        ("base_rate.csv", "722Y001", "M", "0101000", None),
        ("cpi.csv", "901Y009", "M", "0", None),
        ("esi.csv", "513Y001", "M", None, ["ê²½ì œì‹¬ë¦¬ì§€ìˆ˜(ì›ê³„ì—´)"]),
        ("ccsi.csv", "511Y002", "M", None, ["í˜„ì¬ìƒí™œí˜•í¸CSI"]),
        ("construction_bsi_actual.csv", "512Y007", "M", None, ["ì—…í™©ì‹¤ì BSI 1)"]),
        ("construction_bsi_forecast.csv", "512Y008", "M", None, ["ì—…í™©ì „ë§BSI 1)"]),
        ("housing_sale_price.csv", "901Y062", "M", None, ["ì´ì§€ìˆ˜"]),
        ("housing_lease_price.csv", "901Y063", "M", None, ["ì´ì§€ìˆ˜"]),
        ("leading_index.csv", "901Y067", "M", None, ["ì„ í–‰ì§€ìˆ˜ìˆœí™˜ë³€ë™ì¹˜"]),
    ]
    
    # M2 (ì „ê¸°ëŒ€ë¹„ ì¦ê°€ìœ¨)
    m2_files = [("m2_growth.csv", "101Y003", "M", None, ["M2(í‰ì”, ê³„ì ˆì¡°ì •ê³„ì—´)"])]
    
    # ê°œë³„ íŒŒì¼
    individual_files = [
        ("market_rate", "721Y001", "M", None, ["êµ­ê³ ì±„(3ë…„)", "êµ­ê³ ì±„(10ë…„)", "íšŒì‚¬ì±„(3ë…„, AA-)", "íšŒì‚¬ì±„(3ë…„, BBB-)"]),
        ("exchange_usd", "731Y006", "M", None, ["ì›/ë‹¬ëŸ¬(ì¢…ê°€ 15:30)"]),
        ("ppi", "404Y014", "M", None, ["ì² ê°•1ì°¨ì œí’ˆ", "ë¹„ê¸ˆì†ê´‘ë¬¼"]),
        ("import_price", "401Y015", "M", None, ["ì² ê°•1ì°¨ì œí’ˆ", "ë¹„ê¸ˆì†ê´‘ë¬¼"]),
    ]
    
    # ë°ì´í„° ìˆ˜ì§‘ (ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—° ì‹œê°„ í¬í•¨)
    import time
    
    print("=== API ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ===")
    
    for i, (fname, stat, per, item, keys) in enumerate(single_files):
        print(f"\n=== {fname} ìˆ˜ì§‘ ì¤‘ ({i+1}/{len(single_files)}) ===")
        rows = ecos(key, stat, per, start, end, item)
        rows = pick(rows, keys)
        # ecos í´ë” ì•ˆì˜ economic_data í´ë”ì— ì €ì¥
        file_path = os.path.join(economic_data_dir, fname)
        saved_rows = save(file_path, rows)
        all_collected_data.append({"file_name": fname, "data": saved_rows})
        print(f"{fname} ì™„ë£Œ: {len(rows)}ê°œ í–‰ ì €ì¥")
        
        # API ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸° ì‹œê°„
        if i < len(single_files) - 1:  # ë§ˆì§€ë§‰ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
            time.sleep(1)
    
    for i, (fname, stat, per, item, keys) in enumerate(m2_files):
        print(f"\n=== {fname} ìˆ˜ì§‘ ì¤‘ (ì„±ì¥ë¥  ê³„ì‚°) ({i+1}/{len(m2_files)}) ===")
        rows = ecos(key, stat, per, start, end, item)
        rows = pick(rows, keys)
        if rows:
            growth_rows = calculate_growth_rate(rows)
            # ecos í´ë” ì•ˆì˜ economic_data í´ë”ì— ì €ì¥
            file_path = os.path.join(economic_data_dir, fname)
            saved_rows = save(file_path, growth_rows)
            all_collected_data.append({"file_name": fname, "data": saved_rows})
            print(f"{fname} ì™„ë£Œ: {len(growth_rows)}ê°œ í–‰ ì €ì¥")
        else:
            print(f"{fname} ì‹¤íŒ¨: ë°ì´í„° ì—†ìŒ")
        
        # API ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸° ì‹œê°„
        if i < len(m2_files) - 1:
            time.sleep(1)
    
    for i, (base_name, stat, per, item, keys) in enumerate(individual_files):
        print(f"\n=== {base_name} ê°œë³„ íŒŒì¼ ìˆ˜ì§‘ ì¤‘ ({i+1}/{len(individual_files)}) ===")
        rows = ecos(key, stat, per, start, end, item)
        rows = pick(rows, keys)
        saved_rows = save_individual(base_name, rows)
        all_collected_data.append({"file_name": f"{base_name}_files", "data": saved_rows})
        print(f"{base_name} ì™„ë£Œ: {len(saved_rows)}ê°œ í–‰ ì €ì¥")
        
        # API ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸° ì‹œê°„
        if i < len(individual_files) - 1:
            time.sleep(1)
        print(f"\n=== {base_name} ìˆ˜ì§‘ ì¤‘ (ê°œë³„ íŒŒì¼) ===")
        if stat in ["404Y014", "401Y015"]:  # PPI, ìˆ˜ì…ë¬¼ê°€ì§€ìˆ˜
            rows = collect_multiple_items(key, stat, per, start, end, keys)
        else:
            rows = ecos(key, stat, per, start, end, item)
            rows = pick(rows, keys)
        saved_rows = save_individual(base_name, rows)
        all_collected_data.append({"file_name": f"{base_name}.csv", "data": saved_rows})
        print(f"{base_name} ì™„ë£Œ: {len(rows)}ê°œ í–‰ ì²˜ë¦¬")
    
    print("\n=== ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹œì‘ ===")
    
    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
    if save_to_database(all_collected_data):
        print("ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ!")
    else:
        print("ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨ - CSV íŒŒì¼ë§Œ ì‚¬ìš©")
    

    print("\n=== Merged CSV íŒŒì¼ ìƒì„± ì‹œì‘ ===")
    
    # í†µí•© CSV íŒŒì¼ ìƒì„± (DBì™€ ë³„ê°œ)
    if create_merged_csv(all_collected_data):
        print("Merged CSV íŒŒì¼ ìƒì„± ì™„ë£Œ!")
    else:
        print("Merged CSV íŒŒì¼ ìƒì„± ì‹¤íŒ¨")
    
    print("\n=== ëª¨ë“  ì‘ì—… ì™„ë£Œ ===")
    print("ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
